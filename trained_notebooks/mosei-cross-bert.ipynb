{"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"93dc5c3a807e4be9916e56445ade48e5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_73dba3ed5e6244b4bf00b48725c1de3c","IPY_MODEL_5a6c559727624e52b05c0e6eb4bc70ac","IPY_MODEL_a1b9ca12563741589b35d496b49005de"],"layout":"IPY_MODEL_eb9d463fb2e747dc890c285be57e2b66"}},"73dba3ed5e6244b4bf00b48725c1de3c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_562003c246f74afcb75f835d4fb68437","placeholder":"​","style":"IPY_MODEL_c6e5938dd4264c43bf7f748254f1d266","value":"Downloading: 100%"}},"5a6c559727624e52b05c0e6eb4bc70ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f26d398ae5749a898eb4c4f29837e3b","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_91e9eec462d14700a9660d35f391d824","value":231508}},"a1b9ca12563741589b35d496b49005de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_506f0f00f1034639950c350496d72da4","placeholder":"​","style":"IPY_MODEL_13c47bfe630e4ae290f40fb8de0d38e2","value":" 232k/232k [00:00&lt;00:00, 250kB/s]"}},"eb9d463fb2e747dc890c285be57e2b66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"562003c246f74afcb75f835d4fb68437":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6e5938dd4264c43bf7f748254f1d266":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f26d398ae5749a898eb4c4f29837e3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91e9eec462d14700a9660d35f391d824":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"506f0f00f1034639950c350496d72da4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13c47bfe630e4ae290f40fb8de0d38e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7d12367faec4d45aabdcdc7fe42adde":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0a9b0cbf0d2148e1872d3050f826a849","IPY_MODEL_262fc952d8f3493ca3f5329072edade8","IPY_MODEL_8c7717d34a7f4481ae1d43b953a4ac53"],"layout":"IPY_MODEL_672c50c97e864ec294c2f367e781fc50"}},"0a9b0cbf0d2148e1872d3050f826a849":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3358be43c96b4412b2063fb0f8df1de0","placeholder":"​","style":"IPY_MODEL_80c2a373ea344cdca111412affa016d6","value":"Downloading: 100%"}},"262fc952d8f3493ca3f5329072edade8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4385715bbeff4074b52a8c88dd4883d2","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f8888cdb993c4d18ba234de761c6dbf2","value":28}},"8c7717d34a7f4481ae1d43b953a4ac53":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_991379c3b95241489108d6ddecce6110","placeholder":"​","style":"IPY_MODEL_55ddbf63e5164b5380ef265505977a0a","value":" 28.0/28.0 [00:00&lt;00:00, 338B/s]"}},"672c50c97e864ec294c2f367e781fc50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3358be43c96b4412b2063fb0f8df1de0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80c2a373ea344cdca111412affa016d6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4385715bbeff4074b52a8c88dd4883d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8888cdb993c4d18ba234de761c6dbf2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"991379c3b95241489108d6ddecce6110":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55ddbf63e5164b5380ef265505977a0a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac000de20c394403b62bd8ef3a1695d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cdc87320e21b4d60b1e49203d37b291d","IPY_MODEL_4b8a067e70c74360ab8e49c147692560","IPY_MODEL_50b142ff76a54200a31fbf5972275e9a"],"layout":"IPY_MODEL_806f48a45d984bf6b55c88c4782cf2a0"}},"cdc87320e21b4d60b1e49203d37b291d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fe8289b866f4bbfbfbae1804368983d","placeholder":"​","style":"IPY_MODEL_e42f8a40607c41328bbef0a0ed21aed0","value":"Downloading: 100%"}},"4b8a067e70c74360ab8e49c147692560":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b1d2c41e91145baa98c767230b8bf2e","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b9596c9a9dcd42d1be10588e84ac7271","value":570}},"50b142ff76a54200a31fbf5972275e9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7db9662102624eb38b139fbf8b57bd19","placeholder":"​","style":"IPY_MODEL_e70e132ac4b04616bf32a5ae97a1b5ad","value":" 570/570 [00:00&lt;00:00, 6.53kB/s]"}},"806f48a45d984bf6b55c88c4782cf2a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fe8289b866f4bbfbfbae1804368983d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e42f8a40607c41328bbef0a0ed21aed0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b1d2c41e91145baa98c767230b8bf2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9596c9a9dcd42d1be10588e84ac7271":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7db9662102624eb38b139fbf8b57bd19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e70e132ac4b04616bf32a5ae97a1b5ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorch_pretrained_bert --upgrade\n!pip install utils --upgrade\n!pip install urllib3 --upgrade","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M4G-wc9m5-va","outputId":"5757e8a2-ca7a-41c1-acd8-b6f32d5965f7","execution":{"iopub.status.busy":"2022-10-17T19:14:42.080759Z","iopub.execute_input":"2022-10-17T19:14:42.081640Z","iopub.status.idle":"2022-10-17T19:15:17.165784Z","shell.execute_reply.started":"2022-10-17T19:14:42.081551Z","shell.execute_reply":"2022-10-17T19:15:17.164629Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pytorch_pretrained_bert\n  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m313.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from pytorch_pretrained_bert) (2.28.1)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from pytorch_pretrained_bert) (1.24.72)\nRequirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from pytorch_pretrained_bert) (1.11.0)\nRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from pytorch_pretrained_bert) (2021.11.10)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pytorch_pretrained_bert) (4.64.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pytorch_pretrained_bert) (1.21.6)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.3.0)\nRequirement already satisfied: botocore<1.28.0,>=1.27.72 in /opt/conda/lib/python3.7/site-packages (from boto3->pytorch_pretrained_bert) (1.27.72)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->pytorch_pretrained_bert) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3->pytorch_pretrained_bert) (0.6.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->pytorch_pretrained_bert) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->pytorch_pretrained_bert) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->pytorch_pretrained_bert) (2022.6.15.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->pytorch_pretrained_bert) (3.3)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.28.0,>=1.27.72->boto3->pytorch_pretrained_bert) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.72->boto3->pytorch_pretrained_bert) (1.15.0)\nInstalling collected packages: pytorch_pretrained_bert\nSuccessfully installed pytorch_pretrained_bert-0.6.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting utils\n  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\nInstalling collected packages: utils\nSuccessfully installed utils-1.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: urllib3 in /opt/conda/lib/python3.7/site-packages (1.26.12)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom torch.utils.data import Dataset\nimport pickle\nfrom collections import Counter\n\nltoi = {}\nemo_num = 0\n\n\nclass MoseiVideoDataset(Dataset):\n    def __init__(self, annotations_file):\n        self.data = []\n\n        data = pd.read_pickle(annotations_file)\n        max_audio_length = 0\n        \n        global ltoi\n        global emo_num\n\n        for item in data:\n            video_features = item[0]\n            audio_features = item[1]\n            text_features = item[2]\n            emotion = item[3]\n            if emotion not in ltoi:\n                ltoi[emotion] = emo_num\n                emo_num += 1\n\n            emotion = ltoi[emotion]\n\n            self.data.append((text_features, audio_features, video_features, emotion))\n            \n        del data\n            \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        elem = self.data[idx]\n        #      text,    audio,   video,    sentiment\n        return elem[0], elem[1], elem[2], elem[3]\n\n\ntrain_dataset = MoseiVideoDataset('../input/mosei-extracted-features/mosei_train.pkl')\ntest_dataset = MoseiVideoDataset('../input/mosei-extracted-features/mosei_test.pkl')","metadata":{"id":"TTi67_d047LF","execution":{"iopub.status.busy":"2022-10-17T19:15:17.168194Z","iopub.execute_input":"2022-10-17T19:15:17.168897Z","iopub.status.idle":"2022-10-17T19:16:22.314209Z","shell.execute_reply.started":"2022-10-17T19:15:17.168853Z","shell.execute_reply":"2022-10-17T19:16:22.313167Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"ltoi","metadata":{"execution":{"iopub.status.busy":"2022-10-17T19:16:22.315710Z","iopub.execute_input":"2022-10-17T19:16:22.316113Z","iopub.status.idle":"2022-10-17T19:16:22.325453Z","shell.execute_reply.started":"2022-10-17T19:16:22.316074Z","shell.execute_reply":"2022-10-17T19:16:22.323384Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"{'sadness': 0, 'happiness': 1, 'anger': 2, 'disgust': 3}"},"metadata":{}}]},{"cell_type":"code","source":"test_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2022-10-17T17:48:59.536886Z","iopub.execute_input":"2022-10-17T17:48:59.537548Z","iopub.status.idle":"2022-10-17T17:48:59.559383Z","shell.execute_reply.started":"2022-10-17T17:48:59.537512Z","shell.execute_reply":"2022-10-17T17:48:59.558461Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(' I just really left the theater feeling just kind of (uhh)\\n',\n [-275.90854,\n  151.31375,\n  -18.294779,\n  27.321224,\n  -33.04584,\n  32.05427,\n  -2.3669913,\n  22.003483,\n  -15.98027,\n  1.7231718,\n  -3.0374913,\n  1.1150335,\n  10.165657,\n  -10.343684,\n  4.185764,\n  -0.45847028,\n  11.959663,\n  -5.942861,\n  9.595353,\n  6.999398,\n  1.0462515,\n  0.017642803,\n  4.5298705,\n  -0.6053975,\n  -7.309554,\n  4.4720306,\n  -4.9948907,\n  1.8348008,\n  -1.3175384,\n  -0.49223647,\n  2.45144,\n  6.4503846,\n  -3.7358267,\n  -1.7867966,\n  3.0450087,\n  -2.9821837,\n  3.0063972,\n  -4.995261,\n  -0.32349843,\n  2.1837428,\n  19.769285,\n  0.18941644,\n  17.412874,\n  18.326172,\n  21.086178,\n  3.6733036,\n  131.15532,\n  99.398186,\n  63.3318,\n  56.310135,\n  0.9977128,\n  0.47259644,\n  0.52145493,\n  0.90845555,\n  1.3952684,\n  0.8738135,\n  10.038044,\n  4.440038,\n  6.3360705,\n  3.031064,\n  0.64438635,\n  0.5940569,\n  22.830107,\n  0.48927864,\n  6.651941,\n  1.4763132,\n  -3.6165764,\n  -3.5619802,\n  -0.88676876,\n  -11.205779,\n  0.05235013,\n  2.0604782,\n  1.7710075,\n  0.7058161,\n  1.7071753,\n  1.246758,\n  0.73974705,\n  6.007221,\n  17.448086,\n  0.43688855,\n  516.77325,\n  0.32430437,\n  1301.148,\n  0.13343652,\n  -66.02188,\n  -1.3841494,\n  1486.245,\n  0.20396875,\n  1034.3767,\n  0.23248976,\n  -69.9747,\n  -1.270248,\n  2661.4749,\n  0.112364255,\n  1014.9768,\n  0.39604285,\n  -79.065704,\n  -1.0420961,\n  -12.773504,\n  -0.54007584,\n  29.442028,\n  0.20633231,\n  -0.03785615,\n  -0.77146727,\n  -0.007910645,\n  -1.3054196,\n  0.79600906,\n  0.44831365,\n  28.01957,\n  0.26933822,\n  5.1333227,\n  1.9776267,\n  -5.053212,\n  -2.8505201,\n  -2.185145,\n  -4.4747105,\n  -17.244839,\n  31.006094,\n  -0.09736945,\n  -0.0021212895,\n  0.36362284,\n  4.20712,\n  1.9736842,\n  0.32999998,\n  0.17654082,\n  0.15666667,\n  0.12256517,\n  -25.574917],\n [array([[ 72.4011,  73.9988,  76.5856, ..., 147.8667, 151.5674, 151.5674],\n         [ 76.3298,  79.9273,  80.2262, ..., 147.9591, 152.8446, 152.2576],\n         [ 87.8125,  89.7091,  92.5239, ..., 149.9589, 151.9587, 151.9587],\n         ...,\n         [ 11.2484,  35.9578,  52.3368, ...,  61.037 ,  61.6348,  60.8799],\n         [ 15.7318,  29.6272,  39.2133, ...,  52.0918,  59.6889,  52.6896],\n         [ 20.5033,  25.1007,  32.1   , ...,  42.7938,  52.6896,  51.5757]]),\n  array([[ 72.4011,  74.6998,  76.6996, ..., 147.5678, 151.5674, 151.5674],\n         [ 76.3298,  79.9273,  80.2262, ..., 148.5569, 153.1435, 152.2576],\n         [ 87.8125,  89.7091,  92.5239, ..., 149.9589, 151.9587, 151.9587],\n         ...,\n         [ 11.2484,  35.9578,  52.3368, ...,  59.977 ,  61.3359,  60.222 ],\n         [ 15.7318,  29.6272,  39.2133, ...,  51.722 ,  58.7922,  52.6788],\n         [ 20.5033,  25.1007,  32.1   , ...,  42.31  ,  51.7929,  50.679 ]]),\n  array([[ 75.9277,  78.2264,  79.9381, ..., 150.9588, 151.9587, 151.9587],\n         [ 85.8127,  87.5244,  89.9263, ..., 152.9586, 151.9587, 151.9587],\n         [ 93.6764,  97.2739, 100.5016, ..., 154.9584, 155.9583, 155.9583],\n         ...,\n         [ 11.4718,  15.1232,  30.7195, ...,  29.9091,  37.1965,  45.6086],\n         [ 19.0581,  17.596 ,  22.9976, ...,  21.6711,  23.1979,  26.3116],\n         [ 22.2966,  16.0091,  19.4217, ...,  18.3725,  19.3724,  21.3722]]),\n  array([[ 72.8356,  72.7108,  74.1236, ..., 153.0403, 153.0403, 153.0403],\n         [ 74.7645,  78.3512,  80.0521, ..., 153.0403, 153.0403, 153.0403],\n         [ 81.4541,  85.7526,  86.0515, ..., 153.0403, 153.0403, 153.0403],\n         ...,\n         [ 18.1615,  17.1939,  16.6177, ...,  28.0295,  34.0181,  42.6151],\n         [ 23.5631,  24.318 ,  19.0304, ...,  19.4433,  22.0193,  26.7307],\n         [ 23.9159,  20.6712,  13.5579, ...,  20.8884,  21.7959,  21.6819]]),\n  array([[ 72.6399,  74.3408,  75.8137, ..., 149.0299, 151.0297, 151.0297],\n         [ 79.5683,  80.6822,  81.5681, ..., 149.7417, 152.7414, 152.7414],\n         [ 87.2363,  87.9481,  93.1325, ..., 152.0404, 153.0403, 153.0403],\n         ...,\n         [ 23.7202,  44.5009,  48.365 , ...,  61.738 ,  55.6246,  56.9835],\n         [ 20.0303,  31.872 ,  38.5832, ...,  53.728 ,  57.3255,  49.6252],\n         [ 21.9053,  25.9866,  26.1006, ...,  44.9138,  52.326 ,  46.9244]]),\n  array([[ 71.0422,  73.042 ,  74.8739, ..., 152.4317, 152.4317, 152.4317],\n         [ 73.743 ,  76.0417,  77.5747, ..., 152.4317, 152.4317, 152.4317],\n         [ 81.6713,  82.8561,  84.916 , ..., 152.4317, 152.4317, 152.4317],\n         ...,\n         [  3.049 ,  14.4114,  48.3048, ...,  44.316 ,  56.9234,  58.2222],\n         [  9.3796,  23.7202,  44.202 , ...,  35.7298,  45.9245,  54.5215],\n         [ 16.4929,  19.9055,  28.9755, ...,  25.7909,  32.0291,  39.3273]]),\n  array([[ 73.5967,  72.1022,  74.102 , ..., 151.0297, 151.0297, 151.0297],\n         [ 76.9276,  80.0305,  80.0305, ..., 152.4317, 152.4317, 152.4317],\n         [ 84.9977,  87.2255,  87.9974, ..., 152.4317, 152.4317, 152.4317],\n         ...,\n         [ 17.7208,  20.5957,  28.1605, ...,  54.5215,  51.5218,  45.5224],\n         [ 18.6606,  17.7208,  26.7908, ...,  42.6259,  45.2235,  39.523 ],\n         [ 14.661 ,  10.6614,  18.5466, ...,  25.6276,  34.5235,  38.5231]]),\n  array([[ 71.754 ,  70.1563,  71.7432, ..., 151.7307, 152.7306, 152.7306],\n         [ 72.814 ,  73.0913,  76.091 , ..., 152.4317, 152.4317, 152.4317],\n         [ 78.3296,  81.4325,  82.0195, ..., 152.4317, 152.4317, 152.4317],\n         ...,\n         [ 10.7385,  14.7812,  17.7316, ...,  31.5561,  42.327 ,  49.3263],\n         [ 22.5524,  22.6063,  18.9056, ...,  20.9162,  25.9157,  30.0293],\n         [ 22.1934,  21.9592,  14.9599, ...,  19.7853,  19.9594,  19.9594]]),\n  array([[ 71.5691,  70.4552,  72.455 , ..., 151.7307, 152.7306, 152.7306],\n         [ 72.341 ,  73.0913,  76.091 , ..., 152.4317, 152.4317, 152.4317],\n         [ 77.8566,  81.6174,  82.3184, ..., 152.4317, 152.4317, 152.4317],\n         ...,\n         [ 10.7385,  14.7812,  17.7316, ...,  31.5561,  42.327 ,  49.3263],\n         [ 22.5524,  22.6063,  18.9056, ...,  20.9162,  25.9157,  30.0293],\n         [ 22.1934,  21.9592,  14.9599, ...,  19.7853,  19.9594,  19.9594]]),\n  array([[ 74.1514,  75.8523,  78.3467, ..., 152.0404, 154.0402, 154.0402],\n         [ 76.074 ,  78.4867,  80.5682, ..., 153.0403, 154.0402, 154.0402],\n         [ 84.6818,  86.6816,  88.7954, ..., 154.0402, 154.0402, 154.0402],\n         ...,\n         [  5.3477,   8.0315,  33.73  , ...,  35.2676,  47.8642,  57.173 ],\n         [ 14.3791,  17.6176,  31.6162, ...,  29.2682,  36.1643,  43.8754],\n         [ 19.3077,  19.6174,  24.7587, ...,  21.269 ,  23.4645,  26.8771]]),\n  array([[ 71.7432,  73.4118,  75.8954, ..., 153.4316, 153.4316, 153.4316],\n         [ 75.96  ,  79.6715,  81.3123, ..., 153.4316, 153.4316, 153.4316],\n         [ 84.5139,  87.9481,  90.1867, ..., 154.4315, 153.4316, 153.4316],\n         ...,\n         [ 23.4876,  17.2432,  19.5958, ...,  20.6604,  21.2582,  27.6705],\n         [ 19.0797,  20.7205,  15.835 , ...,  19.4864,  20.7852,  22.4861],\n         [ 22.6063,  22.2473,  14.661 , ...,  21.1981,  21.1873,  21.1873]]),\n  array([[ 73.2484,  75.5471,  78.7425, ..., 147.4322, 150.4319, 151.4318],\n         [ 82.8561,  85.3936,  85.7957, ..., 150.4319, 153.4316, 153.4316],\n         [ 90.3069,  92.8444,  93.9583, ..., 152.4317, 153.4316, 153.4316],\n         ...,\n         [ 19.7314,  32.5021,  44.6858, ...,  55.6569,  58.6674,  53.6679],\n         [ 19.1336,  24.2749,  30.3452, ...,  42.6582,  54.6678,  49.9672],\n         [ 15.5469,  22.3891,  27.8015, ...,  31.9582,  38.2673,  44.6688]]),\n  array([[ 72.0528,  74.3407,  77.2372, ..., 147.4322, 150.4319, 151.4318],\n         [ 81.3616,  84.2473,  86.3611, ..., 150.4319, 153.4316, 153.4316],\n         [ 89.4102,  92.0078,  95.2247, ..., 152.4317, 153.4316, 153.4316],\n         ...,\n         [ 19.7314,  32.4913,  44.3653, ...,  54.4613,  57.461 ,  52.7604],\n         [ 19.7314,  24.2641,  29.7366, ...,  42.3485,  53.4614,  48.7608],\n         [ 15.5469,  22.2042,  28.0788, ...,  30.7626,  37.0609,  43.4624]]),\n  array([[ 71.1023,  72.1022,  73.9449, ..., 147.4322, 150.4319, 150.4319],\n         [ 73.4333,  76.319 ,  78.177 , ..., 149.432 , 153.4316, 152.4317],\n         [ 82.8561,  84.1549,  89.2038, ..., 151.4318, 153.4316, 153.4316],\n         ...,\n         [ 17.482 ,  18.7315,  22.688 , ...,  46.6578,  54.6678,  48.9673],\n         [ 18.8347,  16.2587,  20.927 , ...,  32.3711,  39.9682,  42.9679],\n         [ 21.5463,  14.661 ,  13.5471, ...,  20.3723,  29.2682,  35.8546]]),\n  array([[ 72.1946,  73.9664,  76.9769, ..., 151.4318, 153.4316, 153.4316],\n         [ 78.1231,  81.9378,  82.6496, ..., 153.4316, 153.4316, 153.4316],\n         [ 86.5137,  88.7092,  91.41  , ..., 154.4315, 153.4316, 154.4315],\n         ...,\n         [  7.7989,  12.9555,  26.421 , ...,  29.3606,  31.1539,  45.4514],\n         [ 15.4175,  16.4821,  21.7805, ...,  23.7633,  25.1545,  26.877 ],\n         [ 17.9442,  16.7101,  18.7808, ...,  18.8778,  16.5683,  19.4648]]),\n  array([[ 74.9925,  74.5796,  76.6395, ..., 149.4428, 151.0297, 152.0296],\n         [ 82.7036,  84.3614,  86.8342, ..., 150.5567, 151.5566, 152.5565],\n         [ 89.6167,  91.2529,  93.9537, ..., 151.5566, 152.5565, 153.5564],\n         ...,\n         [ 14.0201,  26.6167,  41.2023, ...,  50.4402,  63.3249,  59.6242],\n         [ 18.0197,  20.5033,  28.9324, ...,  38.3274,  55.9235,  55.5214],\n         [ 18.0197,  17.0306,  25.6877, ...,  33.73  ,  37.9253,  47.9243]]),\n  array([[ 74.9925,  74.5796,  76.6395, ..., 149.4428, 151.0297, 152.0296],\n         [ 82.7036,  84.3614,  86.8342, ..., 150.5567, 151.5566, 152.5565],\n         [ 89.6167,  91.2529,  93.9537, ..., 151.5566, 152.5565, 153.5564],\n         ...,\n         [ 14.0201,  26.9156,  42.2022, ...,  50.4402,  63.3249,  59.6242],\n         [ 18.0197,  20.5033,  28.9324, ...,  37.3275,  55.9235,  55.5214],\n         [ 18.0305,  17.6176,  25.5028, ...,  33.73  ,  37.9253,  47.9243]]),\n  array([[ 74.9925,  74.5796,  76.6395, ..., 149.4428, 151.0297, 152.0296],\n         [ 82.7036,  84.3614,  86.8342, ..., 150.5567, 151.5566, 152.5565],\n         [ 89.6167,  91.2529,  93.9537, ..., 151.5566, 152.5565, 153.5564],\n         ...,\n         [ 14.0201,  26.9156,  42.2022, ...,  50.2553,  60.3252,  58.0265],\n         [ 18.0197,  20.5033,  28.9324, ...,  37.3275,  55.3257,  54.6247],\n         [ 18.0197,  17.0306,  25.8017, ...,  34.0289,  37.0286,  47.0276]])],\n 0)"},"metadata":{}}]},{"cell_type":"code","source":"len(train_dataset)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qjDQb1hx5DrJ","outputId":"e213456c-085c-402c-c639-eaea7e99dfbb","execution":{"iopub.status.busy":"2022-10-17T19:16:22.328139Z","iopub.execute_input":"2022-10-17T19:16:22.329249Z","iopub.status.idle":"2022-10-17T19:16:22.338619Z","shell.execute_reply.started":"2022-10-17T19:16:22.329210Z","shell.execute_reply":"2022-10-17T19:16:22.337536Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"8648"},"metadata":{}}]},{"cell_type":"code","source":"len(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-10-17T19:16:22.339898Z","iopub.execute_input":"2022-10-17T19:16:22.340717Z","iopub.status.idle":"2022-10-17T19:16:22.348288Z","shell.execute_reply.started":"2022-10-17T19:16:22.340682Z","shell.execute_reply":"2022-10-17T19:16:22.347125Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"2160"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])","metadata":{"id":"xPtZDqua5t6J","execution":{"iopub.status.busy":"2022-10-17T17:27:13.132303Z","iopub.execute_input":"2022-10-17T17:27:13.135052Z","iopub.status.idle":"2022-10-17T17:27:13.141443Z","shell.execute_reply.started":"2022-10-17T17:27:13.135013Z","shell.execute_reply":"2022-10-17T17:27:13.140510Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Cqosam35uh4","outputId":"c9b3f51c-38d0-429b-cc71-06ce0e198612","execution":{"iopub.status.busy":"2022-10-17T19:16:56.782396Z","iopub.execute_input":"2022-10-17T19:16:56.783007Z","iopub.status.idle":"2022-10-17T19:16:56.881499Z","shell.execute_reply.started":"2022-10-17T19:16:56.782961Z","shell.execute_reply":"2022-10-17T19:16:56.880314Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import classification_report\n\n\ndef multi_metrics(preds, y):\n    _, y_pred_tags = torch.max(preds, dim = 1)\n    _, y_tags = torch.max(y, dim = 1)\n    \n    y_pred_tags = y_pred_tags.cpu().data\n    y_tags = y_tags.cpu().data\n\n    acc = accuracy_score(y_tags, y_pred_tags, normalize=True)\n    prec = precision_score(y_tags, y_pred_tags, average='weighted')\n    f1 = f1_score(y_tags, y_pred_tags, average='weighted')\n    rec = recall_score(y_tags, y_pred_tags, average='weighted')\n    report = classification_report(y_tags, y_pred_tags, output_dict=True)\n    \n    return acc, prec, f1, rec, report\n\n\ndef multi_metrics_for_valid(y_pred_tags, y_tags):\n    acc = accuracy_score(y_tags, y_pred_tags, normalize=True)\n    prec = precision_score(y_tags, y_pred_tags, average='weighted')\n    f1 = f1_score(y_tags, y_pred_tags, average='weighted')\n    rec = recall_score(y_tags, y_pred_tags, average='weighted')\n    report = classification_report(y_tags, y_pred_tags, output_dict=True)\n    \n    return acc, prec, f1, rec, report","metadata":{"id":"SCALgZiJ5yk-","execution":{"iopub.status.busy":"2022-10-17T19:16:58.644863Z","iopub.execute_input":"2022-10-17T19:16:58.645472Z","iopub.status.idle":"2022-10-17T19:16:59.614004Z","shell.execute_reply.started":"2022-10-17T19:16:58.645427Z","shell.execute_reply":"2022-10-17T19:16:59.612651Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, dataloader, criterion):\n    epoch_loss = 0.0\n    y_preds_tags_array = []\n    y_tags_array = []\n    \n    model.eval()\n    \n    with torch.no_grad():\n    \n        for idx, (video, audio, text, attr, labels) in enumerate(dataloader):\n            # features = features.unsqueeze(1)\n            \n            video = video.to(device)\n            text = text.to(device)\n            audio = audio.to(device)\n            attr = attr.to(device)\n            labels = labels.to(device)\n            \n            predictions = model(text, audio, video, attention_mask=attr)[0]\n            \n            # loss = torch.nn.functional.cross_entropy(predictions, labels)\n\n            _, y_pred_tags = torch.max(predictions, dim = 1)\n            _, y_tags = torch.max(labels, dim = 1)\n            y_pred_tags = y_pred_tags.cpu().data\n            y_tags = y_tags.cpu().data\n            y_preds_tags_array.extend(predictions)\n            y_tags_array.extend(labels)\n\n            torch.cuda.empty_cache()\n\n    acc, prec, f1, rec, report = multi_metrics_for_valid(y_preds_tags_array, y_tags_array)\n        \n    return epoch_loss / len(dataloader), acc, prec, f1, rec, report","metadata":{"id":"UqdjO-Ke50dU","execution":{"iopub.status.busy":"2022-10-17T19:17:00.344790Z","iopub.execute_input":"2022-10-17T19:17:00.345782Z","iopub.status.idle":"2022-10-17T19:17:00.356406Z","shell.execute_reply.started":"2022-10-17T19:17:00.345729Z","shell.execute_reply":"2022-10-17T19:17:00.355366Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef train(model, dataloader, optimizer, criterion):\n    epoch_loss = 0\n    epoch_acc = []\n    epoch_prec = []\n    epoch_f1 = []\n    epoch_rec = []\n    report = {}\n    \n    model.train()\n    \n    for idx, (video, audio, text, attr, labels) in enumerate(dataloader):\n        optimizer.zero_grad()\n\n        video = video.to(device)\n        text = text.to(device)\n        audio = audio.to(device)\n        attr = attr.to(device)\n        labels = labels.to(device) \n        \n        loss = model(text, audio, video, attention_mask=attr, labels=labels)\n        loss = loss.mean()\n        loss.backward()\n\n        # print(predictions, type(predictions))\n        # print(labels, type(labels))\n        # print(len(predictions), labels.shape)\n        \n        # print(predictions.shape, labels.shape)\n        # loss = torch.nn.functional.cross_entropy(predictions, labels)\n        # acc, prec, f1, rec, report = multi_metrics(predictions, labels)\n        # loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        # epoch_acc.append(acc)\n        # epoch_prec.append(prec)\n        # epoch_f1.append(f1)\n        # epoch_rec.append(rec)\n\n        torch.cuda.empty_cache()\n\n    return epoch_loss / len(dataloader), epoch_acc, epoch_prec, epoch_f1, epoch_rec, report","metadata":{"id":"VXmafz9f52Oq","execution":{"iopub.status.busy":"2022-10-17T19:17:05.752110Z","iopub.execute_input":"2022-10-17T19:17:05.752483Z","iopub.status.idle":"2022-10-17T19:17:05.763412Z","shell.execute_reply.started":"2022-10-17T19:17:05.752447Z","shell.execute_reply":"2022-10-17T19:17:05.762477Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import gc\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-10-17T19:17:07.982795Z","iopub.execute_input":"2022-10-17T19:17:07.983467Z","iopub.status.idle":"2022-10-17T19:17:08.140109Z","shell.execute_reply.started":"2022-10-17T19:17:07.983431Z","shell.execute_reply":"2022-10-17T19:17:08.139035Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"69"},"metadata":{}}]},{"cell_type":"code","source":"!pip install transformers\nfrom transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":697,"referenced_widgets":["93dc5c3a807e4be9916e56445ade48e5","73dba3ed5e6244b4bf00b48725c1de3c","5a6c559727624e52b05c0e6eb4bc70ac","a1b9ca12563741589b35d496b49005de","eb9d463fb2e747dc890c285be57e2b66","562003c246f74afcb75f835d4fb68437","c6e5938dd4264c43bf7f748254f1d266","4f26d398ae5749a898eb4c4f29837e3b","91e9eec462d14700a9660d35f391d824","506f0f00f1034639950c350496d72da4","13c47bfe630e4ae290f40fb8de0d38e2","f7d12367faec4d45aabdcdc7fe42adde","0a9b0cbf0d2148e1872d3050f826a849","262fc952d8f3493ca3f5329072edade8","8c7717d34a7f4481ae1d43b953a4ac53","672c50c97e864ec294c2f367e781fc50","3358be43c96b4412b2063fb0f8df1de0","80c2a373ea344cdca111412affa016d6","4385715bbeff4074b52a8c88dd4883d2","f8888cdb993c4d18ba234de761c6dbf2","991379c3b95241489108d6ddecce6110","55ddbf63e5164b5380ef265505977a0a","ac000de20c394403b62bd8ef3a1695d5","cdc87320e21b4d60b1e49203d37b291d","4b8a067e70c74360ab8e49c147692560","50b142ff76a54200a31fbf5972275e9a","806f48a45d984bf6b55c88c4782cf2a0","4fe8289b866f4bbfbfbae1804368983d","e42f8a40607c41328bbef0a0ed21aed0","8b1d2c41e91145baa98c767230b8bf2e","b9596c9a9dcd42d1be10588e84ac7271","7db9662102624eb38b139fbf8b57bd19","e70e132ac4b04616bf32a5ae97a1b5ad"]},"id":"BcDku9dm54mb","outputId":"84f6f4a1-9668-4344-980e-6a909777a33e","execution":{"iopub.status.busy":"2022-10-17T19:17:10.288275Z","iopub.execute_input":"2022-10-17T19:17:10.288897Z","iopub.status.idle":"2022-10-17T19:17:29.060112Z","shell.execute_reply.started":"2022-10-17T19:17:10.288848Z","shell.execute_reply":"2022-10-17T19:17:29.059116Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.8.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.12.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63ec86e9d078482e965462b773401628"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"351d8bbc9aa6450b9eebb298a6d85305"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a22ab8831ee4c64af79aebc4997ce18"}},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\n# from nltk.corpus import stopwords\n# import nltk\n# nltk.download('stopwords')\n# stops = set(stopwords.words('english'))\n\n# from string import punctuation \n# import nltk\n# nltk.download('punkt')\n\n# punctuations = list(punctuation)\n# punkt = ['``','...',\"''\",'«','»','…','”','”','“','-','–','..']\n# punctuations.extend(punkt)\n\n\nmax_l = 25\nvideo_lengths = []\ngroup_size = 10\noverlap = 5\n\nBATCH_SIZE = 24\nTEXT_MAX_LENGTH = 50\n\n\ndef collate_batch(batch):\n    label_list, video_list = [], []\n    text_list, att_masks = [], []\n    audio_list = []\n\n    for (_text_features, _audio_features, _video_features, _label) in batch:\n        global max_l\n        global video_lengths\n\n        # label_map = [0.0] * 7\n        # label_map[_label] = 1.0\n        # label_list.append(label_map)\n\n        label_list.append(_label)\n\n        float_f = []\n\n        video_lengths.append(len(_video_features))\n\n        ind = 0\n        while ind < max_l:\n            group = []\n            end_pos = ind + overlap\n\n            for i in range(ind, end_pos):\n                if i >= len(_video_features):\n                    break\n                f = _video_features[i]\n                if f != []:\n#                     f = rgb2gray(f)\n                    pass\n                else:\n                    f = [[0 for col in range(64)] for row in range(64)]\n                f = torch.FloatTensor(f)\n\n                group.append(f)\n            \n            for i in range(len(group), group_size):\n                group.append(torch.zeros(64, 64))\n\n            group = torch.transpose(pad_sequence(group), 0, 1)\n            float_f.append(group)\n            ind += overlap\n            \n        try:\n            X = torch.transpose(pad_sequence(float_f), 0, 1)\n        except Exception:\n            print(len(float_f), float_f[0].shape, float_f[1].shape)\n            raise Exception\n\n        # text = \" \".join([word for word in tokenizer.tokenize(_text_features) if word not in stops and word.isalpha and word not in punctuations])\n        pt = tokenizer(_text_features, padding=\"max_length\", add_special_tokens=True, max_length=TEXT_MAX_LENGTH, return_tensors=\"pt\")\n\n        text_list.append(pt[\"input_ids\"][0][:TEXT_MAX_LENGTH])\n        att_masks.append(pt[\"attention_mask\"][0][:TEXT_MAX_LENGTH])\n        audio_list.append(_audio_features)\n        video_list.append(X)\n\n    # 3d\n    video_features_tensor = torch.transpose(pad_sequence(video_list), 0, 1)\n    \n    audio_tensor = torch.FloatTensor(audio_list)\n    audio_tensor = torch.unsqueeze(audio_tensor, 1)\n\n    # print([t.shape for t in text_list])\n\n    text_tensor = torch.stack(text_list)\n    att_tensor = torch.stack(att_masks)\n\n    label_tensor = torch.FloatTensor(label_list)\n\n    # print(video_features_tensor.shape, audio_tensor.shape, text_tensor.shape)\n\n    return video_features_tensor, audio_tensor, text_tensor, att_tensor, label_tensor\n\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                              shuffle=True, collate_fn=collate_batch)\ndel train_dataset\nvalid_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                              shuffle=True, collate_fn=collate_batch)\ndel test_dataset","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zp2dS5QQ56IO","outputId":"d6a52e1c-53e2-4ecb-a6e5-658b7d163ea0","execution":{"iopub.status.busy":"2022-10-17T19:17:38.595199Z","iopub.execute_input":"2022-10-17T19:17:38.595590Z","iopub.status.idle":"2022-10-17T19:17:38.614985Z","shell.execute_reply.started":"2022-10-17T19:17:38.595554Z","shell.execute_reply":"2022-10-17T19:17:38.614048Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\nfrom pytorch_pretrained_bert.file_utils import cached_path\nimport copy\nimport json\nimport logging\nimport math\nimport os\nimport shutil\nimport torch.nn.functional as F\nimport tarfile\nimport tempfile\nimport sys\nfrom io import open\n\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom utils import *\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_MODEL_ARCHIVE_MAP = {\n    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\",\n    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz\",\n    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz\",\n    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz\",\n    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz\",\n    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz\",\n    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz\",\n}\n\ndef bi_modal_attention(x, y):\n    m1 = torch.matmul(x,y.transpose(-1, -2))\n    n1 = nn.Softmax(dim=-1)(m1)\n    o1 = torch.matmul(n1,y)\n    a1 = torch.mul(o1, x)\n\n    m2 = torch.matmul(y,x.transpose(-1, -2))\n    n2 = nn.Softmax(dim=-1)(m2)\n    o2 = torch.matmul(n2,x)\n    a2 = torch.mul(o2, y)\n    return a1,a2\n\ndef gelu(x):\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n\n\nclass BertConfig(object):\n    def __init__(self,\n                 vocab_size_or_config_json_file,\n                 hidden_size=768,\n                 num_hidden_layers=12,\n                 num_attention_heads=12,\n                 intermediate_size=3072,\n                 hidden_act=\"gelu\",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 type_vocab_size=2,\n                 initializer_range=0.02):\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, \"r\", encoding='utf-8') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.hidden_size = hidden_size\n            self.num_hidden_layers = num_hidden_layers\n            self.num_attention_heads = num_attention_heads\n            self.hidden_act = hidden_act\n            self.intermediate_size = intermediate_size\n            self.hidden_dropout_prob = hidden_dropout_prob\n            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n            self.max_position_embeddings = max_position_embeddings\n            self.type_vocab_size = type_vocab_size\n            self.initializer_range = initializer_range\n        else:\n            raise ValueError(\"First argument must be either a vocabulary size (int)\"\n                             \"or the path to a pretrained model config file (str)\")\n\n    @classmethod\n    def from_dict(cls, json_object):\n        \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n        config = BertConfig(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n        with open(json_file, \"r\", encoding='utf-8') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        \"\"\"Serializes this instance to a JSON string.\"\"\"\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n\ntry:\n    from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm\nexcept ImportError:\n    print(\"Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\")\n    class BertLayerNorm(nn.Module):\n        def __init__(self, hidden_size, eps=1e-12):\n            \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n            \"\"\"\n            super(BertLayerNorm, self).__init__()\n            self.weight = nn.Parameter(torch.ones(hidden_size))\n            self.bias = nn.Parameter(torch.zeros(hidden_size))\n            self.variance_epsilon = eps\n\n        def forward(self, x):\n            u = x.mean(-1, keepdim=True)\n            s = (x - u).pow(2).mean(-1, keepdim=True)\n            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n            return self.weight * x + self.bias\n\nclass BertEmbeddings(nn.Module):\n    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n    \"\"\"\n    def __init__(self, config):\n        super(BertEmbeddings, self).__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, input_ids,token_type_ids=None):\n        seq_length = input_ids.size(1)\n        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n        #在第一维增加一维，`input_ids`: a torch.LongTensor of shape [batch_size, sequence_length] with the word token indices in the vocabulary\n        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        #对应paper中segment embeddings \n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        words_embeddings = self.word_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n        \n        embeddings = words_embeddings + position_embeddings + token_type_embeddings \n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n\n        \nclass BertSelfAttention(nn.Module):\n    def __init__(self, config):\n        super(BertSelfAttention, self).__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                \"The hidden size (%d) is not a multiple of the number of attention \"\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states,audio_data, attention_mask):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n        attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n        return context_layer\n\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super(BertSelfOutput, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states,input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\n\nclass BertAttention(nn.Module):\n    def __init__(self, config):\n        super(BertAttention, self).__init__()\n        self.self = BertSelfAttention(config)\n        self.output = BertSelfOutput(config)\n\n    def forward(self, input_tensor,audio_data,attention_mask):\n        self_output = self.self(input_tensor,audio_data,attention_mask)\n        attention_output = self.output(self_output, input_tensor)\n        return attention_output\n\n\nclass BertIntermediate(nn.Module):\n    def __init__(self, config):\n        super(BertIntermediate, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states\n\n\nclass BertOutput(nn.Module):\n    def __init__(self, config):\n        super(BertOutput, self).__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        \n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertLayer(nn.Module):\n    def __init__(self, config):\n        super(BertLayer, self).__init__()\n        self.attention = BertAttention(config)#过一层Multi-Head Attention 然后与input加和过一层Norm层\n        self.intermediate = BertIntermediate(config)#过一层Linear然后过gelu激活函数\n        self.output = BertOutput(config)#过一层Linear然后过dropout然后与input加和过norm\n\n    def forward(self, hidden_states,all_audio_data,attention_mask):\n        attention_output = self.attention(hidden_states,all_audio_data,attention_mask)\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output\n\n        \nclass BertFinetun(nn.Module):\n    def __init__(self, config):\n        super(BertFinetun, self).__init__()\n        self.proj_t = nn.Conv1d(768,30, kernel_size=1, padding=0, bias=False)\n        self.proj_a = nn.Conv1d(5,30, kernel_size=1, padding=0, bias=False)\n\n        ch1, ch2 = 32, 48\n        k1, k2 = (5, 5, 5), (2, 3, 3)  # 3d kernel size\n        s1, s2 = (2, 2, 2), (2, 2, 2)  # 3d strides\n        pd1, pd2 = (1, 1, 1), (1, 1, 1)  # 3d padding\n        self.proj_v_1 = nn.Conv3d(in_channels=5, out_channels=ch1, \n                                kernel_size=k1, stride=s1,\n                                padding=pd1, bias=False)\n        self.proj_v_2 = nn.Conv3d(in_channels=ch1, out_channels=ch2, \n                                  kernel_size=k2, stride=s2,\n                                  padding=pd2, bias=False)\n        self.bn1 = nn.BatchNorm3d(ch1)\n        self.bn2 = nn.BatchNorm3d(ch2)\n        self.drop_video = nn.Dropout3d(0.1)\n\n        self.fc1 = nn.Linear(36864, 4608)\n        self.fc2 = nn.Linear(4608, 2500)\n\n        # self.fc1 = nn.Linear(36864, 2304)\n        # self.fc2 = nn.Linear(2304, 256)\n        # self.fc3 = nn.Linear(256, 50)\n        \n        # self.fc = nn.Linear(36864, 50)\n\n        self.relu = nn.ReLU(inplace=True)\n\n        self.activation = nn.ReLU()\n        self.audio_weight_1 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n        self.text_weight_1 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n        self.video_weight_1 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n        self.bias = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n        self.audio_weight_1.data.fill_(1)\n        self.text_weight_1.data.fill_(1)\n        self.video_weight_1.data.fill_(1)\n        self.bias.data.fill_(0)\n        self.dropout1 = nn.Dropout(0.3)\n        self.dense = nn.Linear(768, 768)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.LayerNorm1 = BertLayerNorm(768)\n        \n        \n    def forward(self, hidden_states,pooled_output,audio_data,video_data,attention_mask):\n        attention_mask = attention_mask.squeeze(1)\n        attention_mask_ = attention_mask.permute(0, 2, 1)\n        text_data = hidden_states\n        text_data = text_data.transpose(1, 2)\n        text_data = self.proj_t(text_data)\n        text_data = text_data.transpose(1, 2)\n        text_data_1 = text_data.reshape(-1).cpu().detach().numpy()\n        weights = np.sqrt(np.linalg.norm(text_data_1,ord=2))\n        text_data = text_data/weights\n\n        audio_data = audio_data.transpose(1, 2)\n        audio_data = self.proj_a(audio_data)\n        audio_data = audio_data.transpose(1, 2)\n\n        video_data = self.proj_v_1(video_data)\n        video_data = self.bn1(video_data)\n        video_data = self.relu(video_data)\n        video_data = self.drop_video(video_data)\n\n        video_data = self.proj_v_2(video_data)\n        video_data = self.bn2(video_data)\n        video_data = self.relu(video_data)\n        video_data = self.drop_video(video_data)\n\n        video_data = video_data.view(video_data.size(0), -1)\n        video_data = F.relu(self.fc1(video_data))\n        video_data = F.relu(self.fc2(video_data))\n\n        # video_data = F.relu(self.fc1(video_data))\n        # video_data = F.relu(self.fc2(video_data))\n        # video_data = F.relu(self.fc3(video_data))\n\n        # video_data = self.fc1(video_data)\n        # video_data = self.fc2(video_data)\n\n        # video_data = self.fc(video_data)\n        # print(video_data.shape, text_data.shape, audio_data.shape)\n\n        # video_data = video_data.reshape((video_data.shape[0], video_data.shape[1] // 30, 30))\n        \n        # print(video_data.shape, text_data.shape, audio_data.shape)\n\n        # video_data = video_data.unsqueeze(1)\n\n        video_data = video_data.reshape((video_data.shape[0], 50, 50))\n        video_att = video_data\n        video_att = self.activation(video_att)\n        \n        # video_att = torch.matmul(video_data,video_data.transpose(-1, -2))\n        # video_att = self.activation(video_att)\n\n        text_att = torch.matmul(text_data,text_data.transpose(-1, -2))\n        text_att1 = self.activation(text_att)\n\n        audio_att = torch.matmul(audio_data,audio_data.transpose(-1, -2))\n        audio_att = self.activation(audio_att)\n\n        # print(video_att.shape, text_att.shape, audio_att.shape)\n\n        audio_weight_1 = self.audio_weight_1\n        text_weight_1 = self.text_weight_1\n        video_weight_1 = self.video_weight_1\n        bias = self.bias\n        \n        fusion_att = text_weight_1 * text_att1 + audio_weight_1 * audio_att\n        # print(fusion_att.shape, video_weight_1.shape, video_att.shape)\n\n        fusion_att = fusion_att + video_weight_1 * video_att + bias \n\n        fusion_att1 = self.activation(fusion_att)\n        fusion_att = fusion_att+ attention_mask+ attention_mask_\n        fusion_att = nn.Softmax(dim=-1)(fusion_att)\n        fusion_att = self.dropout1(fusion_att)\n        \n        fusion_data = torch.matmul(fusion_att,hidden_states)\n        fusion_data = fusion_data+hidden_states\n\n        hidden_states_new = self.dense(fusion_data)\n        hidden_states_new = self.dropout(hidden_states_new)\n        hidden_states_new = self.LayerNorm1(hidden_states_new)\n        hidden_states_new = hidden_states_new[:,0]\n        return hidden_states_new,text_att1,fusion_att1\n\n\nclass BertEncoder(nn.Module):\n    def __init__(self, config):\n        super(BertEncoder, self).__init__()\n        layer = BertLayer(config)#Transformer block\n        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_attention_heads)])\n\n    def forward(self, hidden_states,all_audio_data,attention_mask, output_all_encoded_layers=True):\n        all_encoder_layers = []\n        for layer_module in self.layer:\n            hidden_states = layer_module(hidden_states,all_audio_data, attention_mask)\n            if output_all_encoded_layers:\n                all_encoder_layers.append(hidden_states)\n        if not output_all_encoded_layers:\n            all_encoder_layers.append(hidden_states)\n        return all_encoder_layers\n\n\nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super(BertPooler, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # We \"pool\" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\n\nclass BertPreTrainedModel(nn.Module):\n    def __init__(self, config, *inputs, **kwargs):\n        super(BertPreTrainedModel, self).__init__()\n        if not isinstance(config, BertConfig):\n            raise ValueError(\n                \"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"\n                \"To create a model from a Google pretrained model use \"\n                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n                    self.__class__.__name__, self.__class__.__name__\n                ))\n        self.config = config\n\n    def init_bert_weights(self, module):\n        \"\"\" Initialize the weights.\n        \"\"\"\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, BertLayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, state_dict=None, cache_dir=None,\n                        from_tf=False, *inputs, **kwargs):\n        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            archive_file = pretrained_model_name_or_path\n        # redirect to the cache, if necessary\n        try:\n            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                \"Model name '{}' was not found in model name list ({}). \"\n                \"We assumed '{}' was a path or url but couldn't find any file \"\n                \"associated to this path or url.\".format(\n                    pretrained_model_name_or_path,\n                    ', '.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n                    archive_file))\n            return None\n        if resolved_archive_file == archive_file:\n            logger.info(\"loading archive file {}\".format(archive_file))\n        else:\n            logger.info(\"loading archive file {} from cache at {}\".format(\n                archive_file, resolved_archive_file))\n        tempdir = None\n        if os.path.isdir(resolved_archive_file) or from_tf:\n            serialization_dir = resolved_archive_file\n        else:\n            # Extract archive to temp dir\n            tempdir = tempfile.mkdtemp()\n            logger.info(\"extracting archive file {} to temp dir {}\".format(\n                resolved_archive_file, tempdir))\n            with tarfile.open(resolved_archive_file, 'r:gz') as archive:\n                archive.extractall(tempdir)\n            serialization_dir = tempdir\n        # Load config\n#         config_file = os.path.join(serialization_dir, CONFIG_NAME)\n        config_file = CONFIG_NAME\n        config = BertConfig.from_json_file(config_file)\n        logger.info(\"Model config {}\".format(config))\n        # Instantiate model.\n        model = cls(config, *inputs, **kwargs)\n        if state_dict is None and not from_tf:\n            weights_path = WEIGHTS_NAME\n            state_dict = torch.load(weights_path, map_location=device)\n        if tempdir:\n            # Clean up temp dir\n            shutil.rmtree(tempdir)\n        # Load from a PyTorch state_dict\n        old_keys = []\n        new_keys = []\n        for key in state_dict.keys():\n            new_key = None\n            if 'gamma' in key:\n                new_key = key.replace('gamma', 'weight')\n            if 'beta' in key:\n                new_key = key.replace('beta', 'bias')\n            if new_key:\n                old_keys.append(key)\n                new_keys.append(new_key)\n        for old_key, new_key in zip(old_keys, new_keys):\n            state_dict[new_key] = state_dict.pop(old_key)\n\n        missing_keys = []\n        unexpected_keys = []\n        error_msgs = []\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, '_metadata', None)\n        state_dict = state_dict.copy()\n        if metadata is not None:\n            state_dict._metadata = metadata\n\n        def load(module, prefix=''):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            module._load_from_state_dict(\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n            for name, child in module._modules.items():\n                if child is not None:\n                    load(child, prefix + name + '.')\n        start_prefix = ''\n        if not hasattr(model, 'bert') and any(s.startswith('bert.') for s in state_dict.keys()):\n            start_prefix = 'bert.'\n        load(model, prefix=start_prefix)\n        if len(missing_keys) > 0:\n            logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(\n                model.__class__.__name__, missing_keys))\n        if len(unexpected_keys) > 0:\n            logger.info(\"Weights from pretrained model not used in {}: {}\".format(\n                model.__class__.__name__, unexpected_keys))\n        if len(error_msgs) > 0:\n            raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n                               model.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n        return model\n\n\nclass BertModel(BertPreTrainedModel):\n    def __init__(self, config):\n        super(BertModel, self).__init__(config)\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = BertEncoder(config)\n        self.pooler = BertPooler(config)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids,all_audio_data,token_type_ids=None, attention_mask=None,output_all_encoded_layers=True):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n        embedding_output = self.embeddings(input_ids,token_type_ids)\n        encoded_layers = self.encoder(embedding_output,\n                                      all_audio_data,\n                                      extended_attention_mask,\n                                      output_all_encoded_layers=output_all_encoded_layers)\n        sequence_output = encoded_layers[-1]#选取最后一层Encoder的输出\n        pooled_output = self.pooler(sequence_output)\n        if not output_all_encoded_layers:\n            encoded_layers = encoded_layers[-1]\n        return sequence_output, pooled_output,extended_attention_mask\n\n\nclass BertForSequenceClassification(BertPreTrainedModel):\n    def __init__(self, config, num_labels):\n        super(BertForSequenceClassification, self).__init__(config)\n        self.num_labels = num_labels\n        self.bert = BertModel(config)\n        self.BertFinetun = BertFinetun(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)#拿出来CLS的表示dimension = 768\n        self.classifier = nn.Linear(config.hidden_size, 1)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, all_audio_data,video_data,token_type_ids=None, attention_mask=None, labels=None):\n        encoder_lastoutput, pooled_output, extend_mask = self.bert(input_ids,all_audio_data,token_type_ids, attention_mask, output_all_encoded_layers=True)\n        pooled_output = self.dropout(pooled_output)\n        pooled_output,text_att,fusion_att = self.BertFinetun(encoder_lastoutput,pooled_output,all_audio_data,video_data,extend_mask)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        if labels is not None:\n            loss = 0.5 * (logits.view(-1) - labels) ** 2 \n            return loss\n        else:\n            return logits,text_att,fusion_att","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rcuNwge058jv","outputId":"7a4ab719-1d10-4ba2-d1ff-024b3272631a","execution":{"iopub.status.busy":"2022-10-17T19:17:43.531830Z","iopub.execute_input":"2022-10-17T19:17:43.532195Z","iopub.status.idle":"2022-10-17T19:17:43.772212Z","shell.execute_reply.started":"2022-10-17T19:17:43.532164Z","shell.execute_reply":"2022-10-17T19:17:43.771156Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n","output_type":"stream"}]},{"cell_type":"code","source":"class BertFinetun(nn.Module):\n    def __init__(self, config):\n        super(BertFinetun, self).__init__()\n        self.proj_t = nn.Conv1d(768,30, kernel_size=1, padding=0, bias=False)\n        self.proj_a = nn.Conv1d(128,30, kernel_size=1, padding=0, bias=False)\n\n        ch1 = 32\n        k1 = (4, 4, 4)  # 3d kernel size\n        s1 = (2, 2, 1)  # 3d strides\n        pd1 = (1, 1, 2)  # 3d padding\n        self.proj_v_1 = nn.Conv3d(in_channels=5, out_channels=ch1, \n                                kernel_size=k1, stride=s1,\n                                padding=pd1, bias=False)\n        self.bn1 = nn.BatchNorm3d(ch1)\n        self.drop_video = nn.Dropout3d(0.1)\n        self.lstm = nn.LSTM(input_size=6656, hidden_size=30, num_layers=2, batch_first=True)\n\n        self.activation = nn.ReLU()\n        self.audio_weight_1 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n        self.text_weight_1 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n        self.video_weight_1 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n        self.bias = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n        self.audio_weight_1.data.fill_(1)\n        self.text_weight_1.data.fill_(1)\n        self.video_weight_1.data.fill_(1)\n        self.bias.data.fill_(0)\n        self.dropout1 = nn.Dropout(0.3)\n        self.dense = nn.Linear(768, 768)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.LayerNorm1 = BertLayerNorm(768)\n        \n        \n    def forward(self, hidden_states,pooled_output,audio_data,video_data,attention_mask):\n        attention_mask = attention_mask.squeeze(1)\n        attention_mask_ = attention_mask.permute(0, 2, 1)\n        text_data = hidden_states\n        text_data = text_data.transpose(1, 2)\n        text_data = self.proj_t(text_data)\n        text_data = text_data.transpose(1, 2)\n        text_data_1 = text_data.reshape(-1).cpu().detach().numpy()\n        weights = np.sqrt(np.linalg.norm(text_data_1,ord=2))\n        text_data = text_data/weights\n\n        audio_data = audio_data.transpose(1, 2)\n        audio_data = self.proj_a(audio_data)\n        audio_data = audio_data.transpose(1, 2)\n\n        video_data = self.proj_v_1(video_data)\n        video_data = self.bn1(video_data)\n\n        # print(video_data.shape)\n        video_data = video_data.reshape((video_data.shape[0], 50, 6656))\n        # video_data = video_data.view(video_data.size(0), -1)\n        # print(video_data.shape)\n\n        video_data, (_, _) = self.lstm(video_data)\n        # print(video_data.shape, text_data.shape, audio_data.shape)\n        \n        video_att = torch.matmul(video_data,video_data.transpose(-1, -2))\n        video_att = self.activation(video_att)\n\n        text_att = torch.matmul(text_data,text_data.transpose(-1, -2))\n        text_att1 = self.activation(text_att)\n\n        audio_att = torch.matmul(audio_data,audio_data.transpose(-1, -2))\n        audio_att = self.activation(audio_att)\n\n        # print(video_att.shape, text_att.shape, audio_att.shape)\n\n        audio_weight_1 = self.audio_weight_1\n        text_weight_1 = self.text_weight_1\n        video_weight_1 = self.video_weight_1\n        bias = self.bias\n        \n        fusion_att = text_weight_1 * text_att1 + audio_weight_1 * audio_att\n        # print(fusion_att.shape, video_weight_1.shape, video_att.shape)\n\n        fusion_att = fusion_att + video_weight_1 * video_att + bias \n\n        fusion_att1 = self.activation(fusion_att)\n        fusion_att = fusion_att+ attention_mask+ attention_mask_\n        fusion_att = nn.Softmax(dim=-1)(fusion_att)\n        fusion_att = self.dropout1(fusion_att)\n        \n        fusion_data = torch.matmul(fusion_att,hidden_states)\n        fusion_data = fusion_data+hidden_states\n\n        hidden_states_new = self.dense(fusion_data)\n        hidden_states_new = self.dropout(hidden_states_new)\n        hidden_states_new = self.LayerNorm1(hidden_states_new)\n        hidden_states_new = hidden_states_new[:,0]\n        return hidden_states_new,text_att1,fusion_att1","metadata":{"id":"Go_g1yot8WOa","execution":{"iopub.status.busy":"2022-10-17T19:17:59.113456Z","iopub.execute_input":"2022-10-17T19:17:59.113841Z","iopub.status.idle":"2022-10-17T19:17:59.132695Z","shell.execute_reply.started":"2022-10-17T19:17:59.113807Z","shell.execute_reply":"2022-10-17T19:17:59.131625Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n!unzip uncased_L-12_H-768_A-12.zip","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vnAsd9C79JqF","outputId":"bcf1fc7e-eafe-4add-972e-2208f3a41bb5","execution":{"iopub.status.busy":"2022-10-17T19:18:03.753393Z","iopub.execute_input":"2022-10-17T19:18:03.753756Z","iopub.status.idle":"2022-10-17T19:18:16.902898Z","shell.execute_reply.started":"2022-10-17T19:18:03.753725Z","shell.execute_reply":"2022-10-17T19:18:16.901706Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"--2022-10-17 19:18:04--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 173.194.210.128, 74.125.141.128, 172.217.204.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|173.194.210.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 407727028 (389M) [application/zip]\nSaving to: ‘uncased_L-12_H-768_A-12.zip’\n\nuncased_L-12_H-768_ 100%[===================>] 388.84M  78.2MB/s    in 5.2s    \n\n2022-10-17 19:18:10 (74.6 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n\nArchive:  uncased_L-12_H-768_A-12.zip\n   creating: uncased_L-12_H-768_A-12/\n  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n","output_type":"stream"}]},{"cell_type":"code","source":"CONFIG_NAME = 'uncased_L-12_H-768_A-12/bert_config.json'\nWEIGHTS_NAME = '../input/pretrained-model/pytorch_model.bin'","metadata":{"id":"ImCCjds49HDt","execution":{"iopub.status.busy":"2022-10-17T19:18:16.905475Z","iopub.execute_input":"2022-10-17T19:18:16.905922Z","iopub.status.idle":"2022-10-17T19:18:16.913678Z","shell.execute_reply.started":"2022-10-17T19:18:16.905879Z","shell.execute_reply":"2022-10-17T19:18:16.912828Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", cache_dir=None, num_labels = 4)\nmodel = model.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lJ33o1XF6LA_","outputId":"257be694-5297-4fd1-dae4-5f28ee83112a","execution":{"iopub.status.busy":"2022-10-17T19:18:23.293636Z","iopub.execute_input":"2022-10-17T19:18:23.294134Z","iopub.status.idle":"2022-10-17T19:28:42.817038Z","shell.execute_reply.started":"2022-10-17T19:18:23.294093Z","shell.execute_reply":"2022-10-17T19:28:42.815987Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"100%|██████████| 407873900/407873900 [10:02<00:00, 677340.31B/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns    \n\n\ndef accuracy_7(out, labels):\n    return np.sum(np.round(out) == np.round(labels)) / float(len(labels))\n\n\ndef multi_metrics_for_valid_with_confusion(y_pred_tags, y_tags):\n    # print(y_pred_tags)\n    # print(y_tags)\n    y_pred_tags = np.round(y_pred_tags)\n    y_tags = np.round(y_tags)\n    acc = accuracy_score(y_tags, y_pred_tags, normalize=True)\n    prec = precision_score(y_tags, y_pred_tags, average='weighted')\n    f1 = f1_score(y_tags, y_pred_tags, average='weighted')\n    rec = recall_score(y_tags, y_pred_tags, average='weighted')\n    report = classification_report(y_tags, y_pred_tags, output_dict=True)\n    labels = ['sadness', 'happiness', 'anger', 'disgust']\n    matrix = confusion_matrix(y_tags, y_pred_tags)\n\n    acc7 = accuracy_7(y_pred_tags, y_tags)\n    \n    return acc, acc7, prec, f1, rec, report, matrix\n\ndef evaluate(model, dataloader, criterion):\n    epoch_loss = 0.0\n    y_preds_tags_array = []\n    y_tags_array = []\n    \n    model.eval()\n    \n    with torch.no_grad():\n    \n        for idx, (video, audio, text, attr, labels) in enumerate(dataloader):\n            # features = features.unsqueeze(1)\n            \n            video = video.to(device)\n            text = text.to(device)\n            audio = audio.to(device)\n            attr = attr.to(device)\n            labels = labels.to(device) \n            \n            predictions = model(text, audio, video, attention_mask=attr)[0]\n            \n            # loss = torch.nn.functional.cross_entropy(predictions, labels)\n\n            # _, y_pred_tags = torch.max(predictions, dim = 1)\n            # _, y_tags = torch.max(labels, dim = 1)\n            # y_pred_tags = y_pred_tags.cpu().data\n            # y_tags = y_tags.cpu().data\n            predictions = predictions.cpu().data\n            predictions = [predictions[i][0].item() for i in range(len(predictions))]\n            labels = labels.cpu().data\n            labels = [labels[i].item() for i in range(len(labels))]\n\n            y_preds_tags_array.extend(predictions)\n            y_tags_array.extend(labels)\n\n            # epoch_loss += loss.item()\n            torch.cuda.empty_cache()\n\n    acc, acc_7, prec, f1, rec, report, matrix = multi_metrics_for_valid_with_confusion(y_preds_tags_array, y_tags_array)\n        \n    return epoch_loss / len(dataloader), acc, acc_7, prec, f1, rec, report, matrix","metadata":{"id":"We0Eo_A46USC","execution":{"iopub.status.busy":"2022-10-17T19:28:42.819284Z","iopub.execute_input":"2022-10-17T19:28:42.820030Z","iopub.status.idle":"2022-10-17T19:28:42.900040Z","shell.execute_reply.started":"2022-10-17T19:28:42.819990Z","shell.execute_reply":"2022-10-17T19:28:42.899181Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\n\nfrom torch import optim\n\nN_EPOCHS = 10\n\noptimizer = optim.Adam(model.parameters(), lr = 2e-6, eps = 1e-8)\ncriterion = nn.BCELoss()\n\nbest_valid_acc = 0.0\nbest_valid_acc_7 = 0.0\nbest_valid_prec = 0.0\nbest_valid_f1 = 0.0\nbest_valid_rec = 0.0\nresult_report = {}\nresult_matrix = {}\n\nfor epoch in tqdm(range(N_EPOCHS)):\n    train_loss, train_acc, train_prec, train_f1, train_rec, _ = train(model, train_dataloader, optimizer, criterion)\n    valid_loss, valid_acc, valid_acc_7, valid_prec, valid_f1, valid_rec, report, matrix = evaluate(model, valid_dataloader, criterion)\n\n    if valid_acc >= best_valid_acc:\n        result_report = report\n        result_matrix = matrix\n    \n    best_valid_acc = max(best_valid_acc, valid_acc)\n    best_valid_acc_7 = max(best_valid_acc_7, valid_acc_7)\n    best_valid_prec = max(best_valid_prec, valid_prec)\n    best_valid_rec = max(best_valid_rec, valid_rec)\n    best_valid_f1 = max(best_valid_f1, valid_f1)\n\n\nprint(f\"Best valid acc = {best_valid_acc}\")\nprint(f\"Best valid acc 7 = {best_valid_acc_7}\")\nprint(f\"Best valid prec = {best_valid_prec}\")\nprint(f\"Best valid f1 = {best_valid_f1}\")\nprint(f\"Best valid rec = {best_valid_rec}\")\nprint(f'Classification report = \\n {json.dumps(result_report, sort_keys=True, indent=4)}')\n\nax = plt.subplot()\nsns.heatmap(result_matrix, annot=True, fmt='g', ax=ax);\nlabels = ['sadness', 'happiness', 'anger', 'disgust']\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"fh0ZyF_xBJ58","outputId":"65c7a92f-a17f-4133-8133-22ea65a4e368","execution":{"iopub.status.busy":"2022-10-17T19:28:42.901403Z","iopub.execute_input":"2022-10-17T19:28:42.901762Z","iopub.status.idle":"2022-10-17T19:48:32.977561Z","shell.execute_reply.started":"2022-10-17T19:28:42.901734Z","shell.execute_reply":"2022-10-17T19:48:32.976625Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"100%|██████████| 10/10 [19:49<00:00, 118.97s/it]\n","output_type":"stream"},{"name":"stdout","text":"Best valid acc = 0.8300925925925926\nBest valid acc 7 = 0.8300925925925926\nBest valid prec = 0.776132505283581\nBest valid f1 = 0.7768779394194955\nBest valid rec = 0.8300925925925926\nClassification report = \n {\n    \"0.0\": {\n        \"f1-score\": 0.9067344345616264,\n        \"precision\": 0.8328664799253035,\n        \"recall\": 0.9949804796430564,\n        \"support\": 1793\n    },\n    \"1.0\": {\n        \"f1-score\": 0.04712041884816753,\n        \"precision\": 0.5,\n        \"recall\": 0.024725274725274724,\n        \"support\": 364\n    },\n    \"2.0\": {\n        \"f1-score\": 0.0,\n        \"precision\": 0.0,\n        \"recall\": 0.0,\n        \"support\": 2\n    },\n    \"3.0\": {\n        \"f1-score\": 0.0,\n        \"precision\": 0.0,\n        \"recall\": 0.0,\n        \"support\": 1\n    },\n    \"accuracy\": 0.8300925925925926,\n    \"macro avg\": {\n        \"f1-score\": 0.23846371335244848,\n        \"precision\": 0.3332166199813259,\n        \"recall\": 0.25492643859208275,\n        \"support\": 2160\n    },\n    \"weighted avg\": {\n        \"f1-score\": 0.7606142007545041,\n        \"precision\": 0.775615554863921,\n        \"recall\": 0.8300925925925926,\n        \"support\": 2160\n    }\n}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\n\nfrom torch import optim\n\nN_EPOCHS = 30\n\noptimizer = optim.Adam(model.parameters(), lr = 2e-6, eps = 1e-8)\ncriterion = nn.BCELoss()\n\nbest_valid_acc = 0.0\nbest_valid_acc_7 = 0.0\nbest_valid_prec = 0.0\nbest_valid_f1 = 0.0\nbest_valid_rec = 0.0\nresult_report = {}\nresult_matrix = {}\n\nfor epoch in tqdm(range(N_EPOCHS)):\n    train_loss, train_acc, train_prec, train_f1, train_rec, _ = train(model, train_dataloader, optimizer, criterion)\n    valid_loss, valid_acc, valid_acc_7, valid_prec, valid_f1, valid_rec, report, matrix = evaluate(model, valid_dataloader, criterion)\n\n    if valid_acc >= best_valid_acc:\n        result_report = report\n        result_matrix = matrix\n    \n    best_valid_acc = max(best_valid_acc, valid_acc)\n    best_valid_acc_7 = max(best_valid_acc_7, valid_acc_7)\n    best_valid_prec = max(best_valid_prec, valid_prec)\n    best_valid_rec = max(best_valid_rec, valid_rec)\n    best_valid_f1 = max(best_valid_f1, valid_f1)\n\n\nprint(f\"Best valid acc = {best_valid_acc}\")\nprint(f\"Best valid acc 7 = {best_valid_acc_7}\")\nprint(f\"Best valid prec = {best_valid_prec}\")\nprint(f\"Best valid f1 = {best_valid_f1}\")\nprint(f\"Best valid rec = {best_valid_rec}\")\nprint(f'Classification report = \\n {json.dumps(result_report, sort_keys=True, indent=4)}')\n\nax = plt.subplot()\nsns.heatmap(result_matrix, annot=True, fmt='g', ax=ax);\nlabels = ['sadness', 'happiness', 'anger', 'disgust']\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}