{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "93dc5c3a807e4be9916e56445ade48e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73dba3ed5e6244b4bf00b48725c1de3c",
              "IPY_MODEL_5a6c559727624e52b05c0e6eb4bc70ac",
              "IPY_MODEL_a1b9ca12563741589b35d496b49005de"
            ],
            "layout": "IPY_MODEL_eb9d463fb2e747dc890c285be57e2b66"
          }
        },
        "73dba3ed5e6244b4bf00b48725c1de3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_562003c246f74afcb75f835d4fb68437",
            "placeholder": "​",
            "style": "IPY_MODEL_c6e5938dd4264c43bf7f748254f1d266",
            "value": "Downloading: 100%"
          }
        },
        "5a6c559727624e52b05c0e6eb4bc70ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f26d398ae5749a898eb4c4f29837e3b",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91e9eec462d14700a9660d35f391d824",
            "value": 231508
          }
        },
        "a1b9ca12563741589b35d496b49005de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_506f0f00f1034639950c350496d72da4",
            "placeholder": "​",
            "style": "IPY_MODEL_13c47bfe630e4ae290f40fb8de0d38e2",
            "value": " 232k/232k [00:00&lt;00:00, 250kB/s]"
          }
        },
        "eb9d463fb2e747dc890c285be57e2b66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "562003c246f74afcb75f835d4fb68437": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6e5938dd4264c43bf7f748254f1d266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f26d398ae5749a898eb4c4f29837e3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91e9eec462d14700a9660d35f391d824": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "506f0f00f1034639950c350496d72da4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13c47bfe630e4ae290f40fb8de0d38e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7d12367faec4d45aabdcdc7fe42adde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a9b0cbf0d2148e1872d3050f826a849",
              "IPY_MODEL_262fc952d8f3493ca3f5329072edade8",
              "IPY_MODEL_8c7717d34a7f4481ae1d43b953a4ac53"
            ],
            "layout": "IPY_MODEL_672c50c97e864ec294c2f367e781fc50"
          }
        },
        "0a9b0cbf0d2148e1872d3050f826a849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3358be43c96b4412b2063fb0f8df1de0",
            "placeholder": "​",
            "style": "IPY_MODEL_80c2a373ea344cdca111412affa016d6",
            "value": "Downloading: 100%"
          }
        },
        "262fc952d8f3493ca3f5329072edade8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4385715bbeff4074b52a8c88dd4883d2",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8888cdb993c4d18ba234de761c6dbf2",
            "value": 28
          }
        },
        "8c7717d34a7f4481ae1d43b953a4ac53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_991379c3b95241489108d6ddecce6110",
            "placeholder": "​",
            "style": "IPY_MODEL_55ddbf63e5164b5380ef265505977a0a",
            "value": " 28.0/28.0 [00:00&lt;00:00, 338B/s]"
          }
        },
        "672c50c97e864ec294c2f367e781fc50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3358be43c96b4412b2063fb0f8df1de0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80c2a373ea344cdca111412affa016d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4385715bbeff4074b52a8c88dd4883d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8888cdb993c4d18ba234de761c6dbf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "991379c3b95241489108d6ddecce6110": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55ddbf63e5164b5380ef265505977a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac000de20c394403b62bd8ef3a1695d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdc87320e21b4d60b1e49203d37b291d",
              "IPY_MODEL_4b8a067e70c74360ab8e49c147692560",
              "IPY_MODEL_50b142ff76a54200a31fbf5972275e9a"
            ],
            "layout": "IPY_MODEL_806f48a45d984bf6b55c88c4782cf2a0"
          }
        },
        "cdc87320e21b4d60b1e49203d37b291d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fe8289b866f4bbfbfbae1804368983d",
            "placeholder": "​",
            "style": "IPY_MODEL_e42f8a40607c41328bbef0a0ed21aed0",
            "value": "Downloading: 100%"
          }
        },
        "4b8a067e70c74360ab8e49c147692560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b1d2c41e91145baa98c767230b8bf2e",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9596c9a9dcd42d1be10588e84ac7271",
            "value": 570
          }
        },
        "50b142ff76a54200a31fbf5972275e9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7db9662102624eb38b139fbf8b57bd19",
            "placeholder": "​",
            "style": "IPY_MODEL_e70e132ac4b04616bf32a5ae97a1b5ad",
            "value": " 570/570 [00:00&lt;00:00, 6.53kB/s]"
          }
        },
        "806f48a45d984bf6b55c88c4782cf2a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fe8289b866f4bbfbfbae1804368983d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e42f8a40607c41328bbef0a0ed21aed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b1d2c41e91145baa98c767230b8bf2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9596c9a9dcd42d1be10588e84ac7271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7db9662102624eb38b139fbf8b57bd19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e70e132ac4b04616bf32a5ae97a1b5ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_pretrained_bert --upgrade\n",
        "!pip install utils --upgrade\n",
        "!pip install urllib3 --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4G-wc9m5-va",
        "outputId": "5757e8a2-ca7a-41c1-acd8-b6f32d5965f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 29.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.24.91-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 69.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.12.1+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.1.1)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting botocore<1.28.0,>=1.27.91\n",
            "  Downloading botocore-1.27.91-py3-none-any.whl (9.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.2 MB 55.5 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 68.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.91->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.91->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2022.9.24)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 74.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.24.91 botocore-1.27.91 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.0 urllib3-1.25.11\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting utils\n",
            "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (1.25.11)\n",
            "Collecting urllib3\n",
            "  Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "Installing collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.25.11\n",
            "    Uninstalling urllib3-1.25.11:\n",
            "      Successfully uninstalled urllib3-1.25.11\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.12 which is incompatible.\u001b[0m\n",
            "Successfully installed urllib3-1.26.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTi67_d047LF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "import pickle\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class MosiVideoDataset(Dataset):\n",
        "    def __init__(self, annotations_file):\n",
        "        self.data = []\n",
        "\n",
        "        data = pd.read_pickle(annotations_file)\n",
        "        max_audio_length = 0\n",
        "\n",
        "        for item in data:\n",
        "            text_features = item[0]\n",
        "            audio_features = item[1]\n",
        "            video_features = item[2]\n",
        "            sentiment = item[3]\n",
        "\n",
        "            self.data.append((text_features, audio_features, video_features, sentiment))\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        elem = self.data[idx]\n",
        "        #      text,    audio,   video,    sentiment\n",
        "        return elem[0], elem[1], elem[2], elem[3]\n",
        "\n",
        "\n",
        "train_dataset = MosiVideoDataset('/content/drive/MyDrive/ИТМО/НИР3/notebooks/mosi_train.pkl')\n",
        "test_dataset = MosiVideoDataset('/content/drive/MyDrive/ИТМО/НИР3/notebooks/mosi_test.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "6bwBGZ77_JfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjDQb1hx5DrJ",
        "outputId": "e213456c-085c-402c-c639-eaea7e99dfbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1970"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def rgb2gray(rgb):\n",
        "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])"
      ],
      "metadata": {
        "id": "xPtZDqua5t6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Cqosam35uh4",
        "outputId": "c9b3f51c-38d0-429b-cc71-06ce0e198612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "def multi_metrics(preds, y):\n",
        "    _, y_pred_tags = torch.max(preds, dim = 1)\n",
        "    _, y_tags = torch.max(y, dim = 1)\n",
        "    \n",
        "    y_pred_tags = y_pred_tags.cpu().data\n",
        "    y_tags = y_tags.cpu().data\n",
        "\n",
        "    acc = accuracy_score(y_tags, y_pred_tags, normalize=True)\n",
        "    prec = precision_score(y_tags, y_pred_tags, average='weighted')\n",
        "    f1 = f1_score(y_tags, y_pred_tags, average='weighted')\n",
        "    rec = recall_score(y_tags, y_pred_tags, average='weighted')\n",
        "    report = classification_report(y_tags, y_pred_tags, output_dict=True)\n",
        "    \n",
        "    return acc, prec, f1, rec, report\n",
        "\n",
        "\n",
        "def multi_metrics_for_valid(y_pred_tags, y_tags):\n",
        "    acc = accuracy_score(y_tags, y_pred_tags, normalize=True)\n",
        "    prec = precision_score(y_tags, y_pred_tags, average='weighted')\n",
        "    f1 = f1_score(y_tags, y_pred_tags, average='weighted')\n",
        "    rec = recall_score(y_tags, y_pred_tags, average='weighted')\n",
        "    report = classification_report(y_tags, y_pred_tags, output_dict=True)\n",
        "    \n",
        "    return acc, prec, f1, rec, report"
      ],
      "metadata": {
        "id": "SCALgZiJ5yk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, criterion):\n",
        "    epoch_loss = 0.0\n",
        "    y_preds_tags_array = []\n",
        "    y_tags_array = []\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for idx, (video, audio, text, attr, labels) in enumerate(dataloader):\n",
        "            # features = features.unsqueeze(1)\n",
        "            \n",
        "            video = video.to(device)\n",
        "            text = text.to(device)\n",
        "            audio = audio.to(device)\n",
        "            attr = attr.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            predictions = model(text, audio, video, attention_mask=attr)[0]\n",
        "            \n",
        "            # loss = torch.nn.functional.cross_entropy(predictions, labels)\n",
        "\n",
        "            _, y_pred_tags = torch.max(predictions, dim = 1)\n",
        "            _, y_tags = torch.max(labels, dim = 1)\n",
        "            y_pred_tags = y_pred_tags.cpu().data\n",
        "            y_tags = y_tags.cpu().data\n",
        "            y_preds_tags_array.extend(predictions)\n",
        "            y_tags_array.extend(labels)\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    acc, prec, f1, rec, report = multi_metrics_for_valid(y_preds_tags_array, y_tags_array)\n",
        "        \n",
        "    return epoch_loss / len(dataloader), acc, prec, f1, rec, report"
      ],
      "metadata": {
        "id": "UqdjO-Ke50dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = []\n",
        "    epoch_prec = []\n",
        "    epoch_f1 = []\n",
        "    epoch_rec = []\n",
        "    report = {}\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for idx, (video, audio, text, attr, labels) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        video = video.to(device)\n",
        "        text = text.to(device)\n",
        "        audio = audio.to(device)\n",
        "        attr = attr.to(device)\n",
        "        labels = labels.to(device) \n",
        "        \n",
        "        loss = model(text, audio, video, attention_mask=attr, labels=labels)\n",
        "        loss = loss.mean()\n",
        "        loss.backward()\n",
        "\n",
        "        # print(predictions, type(predictions))\n",
        "        # print(labels, type(labels))\n",
        "        # print(len(predictions), labels.shape)\n",
        "        \n",
        "        # print(predictions.shape, labels.shape)\n",
        "        # loss = torch.nn.functional.cross_entropy(predictions, labels)\n",
        "        # acc, prec, f1, rec, report = multi_metrics(predictions, labels)\n",
        "        # loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        # epoch_acc.append(acc)\n",
        "        # epoch_prec.append(prec)\n",
        "        # epoch_f1.append(f1)\n",
        "        # epoch_rec.append(rec)\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return epoch_loss / len(dataloader), epoch_acc, epoch_prec, epoch_f1, epoch_rec, report"
      ],
      "metadata": {
        "id": "VXmafz9f52Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697,
          "referenced_widgets": [
            "93dc5c3a807e4be9916e56445ade48e5",
            "73dba3ed5e6244b4bf00b48725c1de3c",
            "5a6c559727624e52b05c0e6eb4bc70ac",
            "a1b9ca12563741589b35d496b49005de",
            "eb9d463fb2e747dc890c285be57e2b66",
            "562003c246f74afcb75f835d4fb68437",
            "c6e5938dd4264c43bf7f748254f1d266",
            "4f26d398ae5749a898eb4c4f29837e3b",
            "91e9eec462d14700a9660d35f391d824",
            "506f0f00f1034639950c350496d72da4",
            "13c47bfe630e4ae290f40fb8de0d38e2",
            "f7d12367faec4d45aabdcdc7fe42adde",
            "0a9b0cbf0d2148e1872d3050f826a849",
            "262fc952d8f3493ca3f5329072edade8",
            "8c7717d34a7f4481ae1d43b953a4ac53",
            "672c50c97e864ec294c2f367e781fc50",
            "3358be43c96b4412b2063fb0f8df1de0",
            "80c2a373ea344cdca111412affa016d6",
            "4385715bbeff4074b52a8c88dd4883d2",
            "f8888cdb993c4d18ba234de761c6dbf2",
            "991379c3b95241489108d6ddecce6110",
            "55ddbf63e5164b5380ef265505977a0a",
            "ac000de20c394403b62bd8ef3a1695d5",
            "cdc87320e21b4d60b1e49203d37b291d",
            "4b8a067e70c74360ab8e49c147692560",
            "50b142ff76a54200a31fbf5972275e9a",
            "806f48a45d984bf6b55c88c4782cf2a0",
            "4fe8289b866f4bbfbfbae1804368983d",
            "e42f8a40607c41328bbef0a0ed21aed0",
            "8b1d2c41e91145baa98c767230b8bf2e",
            "b9596c9a9dcd42d1be10588e84ac7271",
            "7db9662102624eb38b139fbf8b57bd19",
            "e70e132ac4b04616bf32a5ae97a1b5ad"
          ]
        },
        "id": "BcDku9dm54mb",
        "outputId": "84f6f4a1-9668-4344-980e-6a909777a33e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 28.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 50.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 68.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (5.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: urllib3, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.12\n",
            "    Uninstalling urllib3-1.26.12:\n",
            "      Successfully uninstalled urllib3-1.26.12\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1 urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93dc5c3a807e4be9916e56445ade48e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7d12367faec4d45aabdcdc7fe42adde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac000de20c394403b62bd8ef3a1695d5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "from string import punctuation \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "punctuations = list(punctuation)\n",
        "punkt = ['``','...',\"''\",'«','»','…','”','”','“','-','–','..']\n",
        "punctuations.extend(punkt)\n",
        "\n",
        "\n",
        "max_l = 25\n",
        "video_lengths = []\n",
        "group_size = 10\n",
        "overlap = 5\n",
        "\n",
        "BATCH_SIZE = 24\n",
        "TEXT_MAX_LENGTH = 50\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, video_list = [], []\n",
        "    text_list, att_masks = [], []\n",
        "    audio_list = []\n",
        "\n",
        "    for (_text_features, _audio_features, _video_features, _label) in batch:\n",
        "        global max_l\n",
        "        global video_lengths\n",
        "\n",
        "        # label_map = [0.0] * 7\n",
        "        # label_map[_label] = 1.0\n",
        "        # label_list.append(label_map)\n",
        "\n",
        "        label_list.append(_label)\n",
        "\n",
        "        float_f = []\n",
        "\n",
        "        video_lengths.append(len(_video_features))\n",
        "\n",
        "        ind = 0\n",
        "        while ind < max_l:\n",
        "            group = []\n",
        "            end_pos = ind + overlap\n",
        "\n",
        "            for i in range(ind, end_pos):\n",
        "                if i >= len(_video_features):\n",
        "                    break\n",
        "                f = _video_features[i]\n",
        "                if f != []:\n",
        "                    f = rgb2gray(f)\n",
        "                else:\n",
        "                    f = [[0 for col in range(64)] for row in range(64)]\n",
        "                f = torch.FloatTensor(f)\n",
        "\n",
        "                group.append(f)\n",
        "            \n",
        "            for i in range(len(group), group_size):\n",
        "                group.append(torch.zeros(64, 64))\n",
        "\n",
        "            group = torch.transpose(pad_sequence(group), 0, 1)\n",
        "            float_f.append(group)\n",
        "            ind += overlap\n",
        "            \n",
        "        try:\n",
        "            X = torch.transpose(pad_sequence(float_f), 0, 1)\n",
        "        except Exception:\n",
        "            print(len(float_f), float_f[0].shape, float_f[1].shape)\n",
        "            raise Exception\n",
        "\n",
        "        # text = \" \".join([word for word in tokenizer.tokenize(_text_features) if word not in stops and word.isalpha and word not in punctuations])\n",
        "        pt = tokenizer(_text_features, padding=\"max_length\", add_special_tokens=True, max_length=TEXT_MAX_LENGTH, return_tensors=\"pt\")\n",
        "\n",
        "        text_list.append(pt[\"input_ids\"][0][:TEXT_MAX_LENGTH])\n",
        "        att_masks.append(pt[\"attention_mask\"][0][:TEXT_MAX_LENGTH])\n",
        "        audio_list.append(_audio_features)\n",
        "        video_list.append(X)\n",
        "\n",
        "    # 3d\n",
        "    video_features_tensor = torch.transpose(pad_sequence(video_list), 0, 1)\n",
        "    \n",
        "    audio_tensor = torch.FloatTensor(audio_list)\n",
        "    audio_tensor = torch.unsqueeze(audio_tensor, 1)\n",
        "\n",
        "    # print([t.shape for t in text_list])\n",
        "\n",
        "    text_tensor = torch.stack(text_list)\n",
        "    att_tensor = torch.stack(att_masks)\n",
        "\n",
        "    label_tensor = torch.FloatTensor(label_list)\n",
        "\n",
        "    # print(video_features_tensor.shape, audio_tensor.shape, text_tensor.shape)\n",
        "\n",
        "    return video_features_tensor, audio_tensor, text_tensor, att_tensor, label_tensor\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp2dS5QQ56IO",
        "outputId": "d6a52e1c-53e2-4ecb-a6e5-658b7d163ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from pytorch_pretrained_bert.file_utils import cached_path\n",
        "import copy\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import shutil\n",
        "import torch.nn.functional as F\n",
        "import tarfile\n",
        "import tempfile\n",
        "import sys\n",
        "from io import open\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from utils import *\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "PRETRAINED_MODEL_ARCHIVE_MAP = {\n",
        "    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\",\n",
        "    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz\",\n",
        "    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz\",\n",
        "    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz\",\n",
        "    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz\",\n",
        "    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz\",\n",
        "    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz\",\n",
        "}\n",
        "\n",
        "def bi_modal_attention(x, y):\n",
        "    m1 = torch.matmul(x,y.transpose(-1, -2))\n",
        "    n1 = nn.Softmax(dim=-1)(m1)\n",
        "    o1 = torch.matmul(n1,y)\n",
        "    a1 = torch.mul(o1, x)\n",
        "\n",
        "    m2 = torch.matmul(y,x.transpose(-1, -2))\n",
        "    n2 = nn.Softmax(dim=-1)(m2)\n",
        "    o2 = torch.matmul(n2,x)\n",
        "    a2 = torch.mul(o2, y)\n",
        "    return a1,a2\n",
        "\n",
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
        "\n",
        "\n",
        "class BertConfig(object):\n",
        "    def __init__(self,\n",
        "                 vocab_size_or_config_json_file,\n",
        "                 hidden_size=768,\n",
        "                 num_hidden_layers=12,\n",
        "                 num_attention_heads=12,\n",
        "                 intermediate_size=3072,\n",
        "                 hidden_act=\"gelu\",\n",
        "                 hidden_dropout_prob=0.1,\n",
        "                 attention_probs_dropout_prob=0.1,\n",
        "                 max_position_embeddings=512,\n",
        "                 type_vocab_size=2,\n",
        "                 initializer_range=0.02):\n",
        "        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n",
        "                        and isinstance(vocab_size_or_config_json_file, unicode)):\n",
        "            with open(vocab_size_or_config_json_file, \"r\", encoding='utf-8') as reader:\n",
        "                json_config = json.loads(reader.read())\n",
        "            for key, value in json_config.items():\n",
        "                self.__dict__[key] = value\n",
        "        elif isinstance(vocab_size_or_config_json_file, int):\n",
        "            self.vocab_size = vocab_size_or_config_json_file\n",
        "            self.hidden_size = hidden_size\n",
        "            self.num_hidden_layers = num_hidden_layers\n",
        "            self.num_attention_heads = num_attention_heads\n",
        "            self.hidden_act = hidden_act\n",
        "            self.intermediate_size = intermediate_size\n",
        "            self.hidden_dropout_prob = hidden_dropout_prob\n",
        "            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "            self.max_position_embeddings = max_position_embeddings\n",
        "            self.type_vocab_size = type_vocab_size\n",
        "            self.initializer_range = initializer_range\n",
        "        else:\n",
        "            raise ValueError(\"First argument must be either a vocabulary size (int)\"\n",
        "                             \"or the path to a pretrained model config file (str)\")\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, json_object):\n",
        "        \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n",
        "        config = BertConfig(vocab_size_or_config_json_file=-1)\n",
        "        for key, value in json_object.items():\n",
        "            config.__dict__[key] = value\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_json_file(cls, json_file):\n",
        "        \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
        "        with open(json_file, \"r\", encoding='utf-8') as reader:\n",
        "            text = reader.read()\n",
        "        return cls.from_dict(json.loads(text))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "try:\n",
        "    from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm\n",
        "except ImportError:\n",
        "    print(\"Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\")\n",
        "    class BertLayerNorm(nn.Module):\n",
        "        def __init__(self, hidden_size, eps=1e-12):\n",
        "            \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
        "            \"\"\"\n",
        "            super(BertLayerNorm, self).__init__()\n",
        "            self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "            self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "            self.variance_epsilon = eps\n",
        "\n",
        "        def forward(self, x):\n",
        "            u = x.mean(-1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "            return self.weight * x + self.bias\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertEmbeddings, self).__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids,token_type_ids=None):\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "        #在第一维增加一维，`input_ids`: a torch.LongTensor of shape [batch_size, sequence_length] with the word token indices in the vocabulary\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "        #对应paper中segment embeddings \n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        words_embeddings = self.word_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        \n",
        "        embeddings = words_embeddings + position_embeddings + token_type_embeddings \n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "        \n",
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfAttention, self).__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states,audio_data, attention_mask):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        return context_layer\n",
        "\n",
        "\n",
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states,input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertAttention, self).__init__()\n",
        "        self.self = BertSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "\n",
        "    def forward(self, input_tensor,audio_data,attention_mask):\n",
        "        self_output = self.self(input_tensor,audio_data,attention_mask)\n",
        "        attention_output = self.output(self_output, input_tensor)\n",
        "        return attention_output\n",
        "\n",
        "\n",
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertIntermediate, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        \n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertLayer, self).__init__()\n",
        "        self.attention = BertAttention(config)#过一层Multi-Head Attention 然后与input加和过一层Norm层\n",
        "        self.intermediate = BertIntermediate(config)#过一层Linear然后过gelu激活函数\n",
        "        self.output = BertOutput(config)#过一层Linear然后过dropout然后与input加和过norm\n",
        "\n",
        "    def forward(self, hidden_states,all_audio_data,attention_mask):\n",
        "        attention_output = self.attention(hidden_states,all_audio_data,attention_mask)\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "        \n",
        "class BertFinetun(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertFinetun, self).__init__()\n",
        "        self.proj_t = nn.Conv1d(768,30, kernel_size=1, padding=0, bias=False)\n",
        "        self.proj_a = nn.Conv1d(5,30, kernel_size=1, padding=0, bias=False)\n",
        "\n",
        "        ch1, ch2 = 32, 48\n",
        "        k1, k2 = (5, 5, 5), (2, 3, 3)  # 3d kernel size\n",
        "        s1, s2 = (2, 2, 2), (2, 2, 2)  # 3d strides\n",
        "        pd1, pd2 = (1, 1, 1), (1, 1, 1)  # 3d padding\n",
        "        self.proj_v_1 = nn.Conv3d(in_channels=5, out_channels=ch1, \n",
        "                                kernel_size=k1, stride=s1,\n",
        "                                padding=pd1, bias=False)\n",
        "        self.proj_v_2 = nn.Conv3d(in_channels=ch1, out_channels=ch2, \n",
        "                                  kernel_size=k2, stride=s2,\n",
        "                                  padding=pd2, bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(ch1)\n",
        "        self.bn2 = nn.BatchNorm3d(ch2)\n",
        "        self.drop_video = nn.Dropout3d(0.1)\n",
        "\n",
        "        self.fc1 = nn.Linear(36864, 4608)\n",
        "        self.fc2 = nn.Linear(4608, 2500)\n",
        "\n",
        "        # self.fc1 = nn.Linear(36864, 2304)\n",
        "        # self.fc2 = nn.Linear(2304, 256)\n",
        "        # self.fc3 = nn.Linear(256, 50)\n",
        "        \n",
        "        # self.fc = nn.Linear(36864, 50)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "        self.audio_weight_1 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n",
        "        self.text_weight_1 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n",
        "        self.video_weight_1 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n",
        "        self.bias = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n",
        "        self.audio_weight_1.data.fill_(1)\n",
        "        self.text_weight_1.data.fill_(1)\n",
        "        self.video_weight_1.data.fill_(1)\n",
        "        self.bias.data.fill_(0)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.dense = nn.Linear(768, 768)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.LayerNorm1 = BertLayerNorm(768)\n",
        "        \n",
        "        \n",
        "    def forward(self, hidden_states,pooled_output,audio_data,video_data,attention_mask):\n",
        "        attention_mask = attention_mask.squeeze(1)\n",
        "        attention_mask_ = attention_mask.permute(0, 2, 1)\n",
        "        text_data = hidden_states\n",
        "        text_data = text_data.transpose(1, 2)\n",
        "        text_data = self.proj_t(text_data)\n",
        "        text_data = text_data.transpose(1, 2)\n",
        "        text_data_1 = text_data.reshape(-1).cpu().detach().numpy()\n",
        "        weights = np.sqrt(np.linalg.norm(text_data_1,ord=2))\n",
        "        text_data = text_data/weights\n",
        "\n",
        "        audio_data = audio_data.transpose(1, 2)\n",
        "        audio_data = self.proj_a(audio_data)\n",
        "        audio_data = audio_data.transpose(1, 2)\n",
        "\n",
        "        video_data = self.proj_v_1(video_data)\n",
        "        video_data = self.bn1(video_data)\n",
        "        video_data = self.relu(video_data)\n",
        "        video_data = self.drop_video(video_data)\n",
        "\n",
        "        video_data = self.proj_v_2(video_data)\n",
        "        video_data = self.bn2(video_data)\n",
        "        video_data = self.relu(video_data)\n",
        "        video_data = self.drop_video(video_data)\n",
        "\n",
        "        video_data = video_data.view(video_data.size(0), -1)\n",
        "        video_data = F.relu(self.fc1(video_data))\n",
        "        video_data = F.relu(self.fc2(video_data))\n",
        "\n",
        "        # video_data = F.relu(self.fc1(video_data))\n",
        "        # video_data = F.relu(self.fc2(video_data))\n",
        "        # video_data = F.relu(self.fc3(video_data))\n",
        "\n",
        "        # video_data = self.fc1(video_data)\n",
        "        # video_data = self.fc2(video_data)\n",
        "\n",
        "        # video_data = self.fc(video_data)\n",
        "        # print(video_data.shape, text_data.shape, audio_data.shape)\n",
        "\n",
        "        # video_data = video_data.reshape((video_data.shape[0], video_data.shape[1] // 30, 30))\n",
        "        \n",
        "        # print(video_data.shape, text_data.shape, audio_data.shape)\n",
        "\n",
        "        # video_data = video_data.unsqueeze(1)\n",
        "\n",
        "        video_data = video_data.reshape((video_data.shape[0], 50, 50))\n",
        "        video_att = video_data\n",
        "        video_att = self.activation(video_att)\n",
        "        \n",
        "        # video_att = torch.matmul(video_data,video_data.transpose(-1, -2))\n",
        "        # video_att = self.activation(video_att)\n",
        "\n",
        "        text_att = torch.matmul(text_data,text_data.transpose(-1, -2))\n",
        "        text_att1 = self.activation(text_att)\n",
        "\n",
        "        audio_att = torch.matmul(audio_data,audio_data.transpose(-1, -2))\n",
        "        audio_att = self.activation(audio_att)\n",
        "\n",
        "        # print(video_att.shape, text_att.shape, audio_att.shape)\n",
        "\n",
        "        audio_weight_1 = self.audio_weight_1\n",
        "        text_weight_1 = self.text_weight_1\n",
        "        video_weight_1 = self.video_weight_1\n",
        "        bias = self.bias\n",
        "        \n",
        "        fusion_att = text_weight_1 * text_att1 + audio_weight_1 * audio_att\n",
        "        # print(fusion_att.shape, video_weight_1.shape, video_att.shape)\n",
        "\n",
        "        fusion_att = fusion_att + video_weight_1 * video_att + bias \n",
        "\n",
        "        fusion_att1 = self.activation(fusion_att)\n",
        "        fusion_att = fusion_att+ attention_mask+ attention_mask_\n",
        "        fusion_att = nn.Softmax(dim=-1)(fusion_att)\n",
        "        fusion_att = self.dropout1(fusion_att)\n",
        "        \n",
        "        fusion_data = torch.matmul(fusion_att,hidden_states)\n",
        "        fusion_data = fusion_data+hidden_states\n",
        "\n",
        "        hidden_states_new = self.dense(fusion_data)\n",
        "        hidden_states_new = self.dropout(hidden_states_new)\n",
        "        hidden_states_new = self.LayerNorm1(hidden_states_new)\n",
        "        hidden_states_new = hidden_states_new[:,0]\n",
        "        return hidden_states_new,text_att1,fusion_att1\n",
        "\n",
        "\n",
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertEncoder, self).__init__()\n",
        "        layer = BertLayer(config)#Transformer block\n",
        "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_attention_heads)])\n",
        "\n",
        "    def forward(self, hidden_states,all_audio_data,attention_mask, output_all_encoded_layers=True):\n",
        "        all_encoder_layers = []\n",
        "        for layer_module in self.layer:\n",
        "            hidden_states = layer_module(hidden_states,all_audio_data, attention_mask)\n",
        "            if output_all_encoded_layers:\n",
        "                all_encoder_layers.append(hidden_states)\n",
        "        if not output_all_encoded_layers:\n",
        "            all_encoder_layers.append(hidden_states)\n",
        "        return all_encoder_layers\n",
        "\n",
        "\n",
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertPooler, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class BertPreTrainedModel(nn.Module):\n",
        "    def __init__(self, config, *inputs, **kwargs):\n",
        "        super(BertPreTrainedModel, self).__init__()\n",
        "        if not isinstance(config, BertConfig):\n",
        "            raise ValueError(\n",
        "                \"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"\n",
        "                \"To create a model from a Google pretrained model use \"\n",
        "                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n",
        "                    self.__class__.__name__, self.__class__.__name__\n",
        "                ))\n",
        "        self.config = config\n",
        "\n",
        "    def init_bert_weights(self, module):\n",
        "        \"\"\" Initialize the weights.\n",
        "        \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        elif isinstance(module, BertLayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, state_dict=None, cache_dir=None,\n",
        "                        from_tf=False, *inputs, **kwargs):\n",
        "        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n",
        "            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n",
        "        else:\n",
        "            archive_file = pretrained_model_name_or_path\n",
        "        # redirect to the cache, if necessary\n",
        "        try:\n",
        "            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n",
        "        except EnvironmentError:\n",
        "            logger.error(\n",
        "                \"Model name '{}' was not found in model name list ({}). \"\n",
        "                \"We assumed '{}' was a path or url but couldn't find any file \"\n",
        "                \"associated to this path or url.\".format(\n",
        "                    pretrained_model_name_or_path,\n",
        "                    ', '.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n",
        "                    archive_file))\n",
        "            return None\n",
        "        if resolved_archive_file == archive_file:\n",
        "            logger.info(\"loading archive file {}\".format(archive_file))\n",
        "        else:\n",
        "            logger.info(\"loading archive file {} from cache at {}\".format(\n",
        "                archive_file, resolved_archive_file))\n",
        "        tempdir = None\n",
        "        if os.path.isdir(resolved_archive_file) or from_tf:\n",
        "            serialization_dir = resolved_archive_file\n",
        "        else:\n",
        "            # Extract archive to temp dir\n",
        "            tempdir = tempfile.mkdtemp()\n",
        "            logger.info(\"extracting archive file {} to temp dir {}\".format(\n",
        "                resolved_archive_file, tempdir))\n",
        "            with tarfile.open(resolved_archive_file, 'r:gz') as archive:\n",
        "                archive.extractall(tempdir)\n",
        "            serialization_dir = tempdir\n",
        "        # Load config\n",
        "        config_file = os.path.join(serialization_dir, CONFIG_NAME)\n",
        "        config = BertConfig.from_json_file(config_file)\n",
        "        logger.info(\"Model config {}\".format(config))\n",
        "        # Instantiate model.\n",
        "        model = cls(config, *inputs, **kwargs)\n",
        "        if state_dict is None and not from_tf:\n",
        "            weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n",
        "            state_dict = torch.load(weights_path, map_location=device)\n",
        "        if tempdir:\n",
        "            # Clean up temp dir\n",
        "            shutil.rmtree(tempdir)\n",
        "        # Load from a PyTorch state_dict\n",
        "        old_keys = []\n",
        "        new_keys = []\n",
        "        for key in state_dict.keys():\n",
        "            new_key = None\n",
        "            if 'gamma' in key:\n",
        "                new_key = key.replace('gamma', 'weight')\n",
        "            if 'beta' in key:\n",
        "                new_key = key.replace('beta', 'bias')\n",
        "            if new_key:\n",
        "                old_keys.append(key)\n",
        "                new_keys.append(new_key)\n",
        "        for old_key, new_key in zip(old_keys, new_keys):\n",
        "            state_dict[new_key] = state_dict.pop(old_key)\n",
        "\n",
        "        missing_keys = []\n",
        "        unexpected_keys = []\n",
        "        error_msgs = []\n",
        "        # copy state_dict so _load_from_state_dict can modify it\n",
        "        metadata = getattr(state_dict, '_metadata', None)\n",
        "        state_dict = state_dict.copy()\n",
        "        if metadata is not None:\n",
        "            state_dict._metadata = metadata\n",
        "\n",
        "        def load(module, prefix=''):\n",
        "            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
        "            module._load_from_state_dict(\n",
        "                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
        "            for name, child in module._modules.items():\n",
        "                if child is not None:\n",
        "                    load(child, prefix + name + '.')\n",
        "        start_prefix = ''\n",
        "        if not hasattr(model, 'bert') and any(s.startswith('bert.') for s in state_dict.keys()):\n",
        "            start_prefix = 'bert.'\n",
        "        load(model, prefix=start_prefix)\n",
        "        if len(missing_keys) > 0:\n",
        "            logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(\n",
        "                model.__class__.__name__, missing_keys))\n",
        "        if len(unexpected_keys) > 0:\n",
        "            logger.info(\"Weights from pretrained model not used in {}: {}\".format(\n",
        "                model.__class__.__name__, unexpected_keys))\n",
        "        if len(error_msgs) > 0:\n",
        "            raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
        "                               model.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
        "        return model\n",
        "\n",
        "\n",
        "class BertModel(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super(BertModel, self).__init__(config)\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config)\n",
        "        self.pooler = BertPooler(config)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids,all_audio_data,token_type_ids=None, attention_mask=None,output_all_encoded_layers=True):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\n",
        "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "        embedding_output = self.embeddings(input_ids,token_type_ids)\n",
        "        encoded_layers = self.encoder(embedding_output,\n",
        "                                      all_audio_data,\n",
        "                                      extended_attention_mask,\n",
        "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
        "        sequence_output = encoded_layers[-1]#选取最后一层Encoder的输出\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        if not output_all_encoded_layers:\n",
        "            encoded_layers = encoded_layers[-1]\n",
        "        return sequence_output, pooled_output,extended_attention_mask\n",
        "\n",
        "\n",
        "class BertForSequenceClassification(BertPreTrainedModel):\n",
        "    def __init__(self, config, num_labels):\n",
        "        super(BertForSequenceClassification, self).__init__(config)\n",
        "        self.num_labels = num_labels\n",
        "        self.bert = BertModel(config)\n",
        "        self.BertFinetun = BertFinetun(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)#拿出来CLS的表示dimension = 768\n",
        "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, all_audio_data,video_data,token_type_ids=None, attention_mask=None, labels=None):\n",
        "        encoder_lastoutput, pooled_output, extend_mask = self.bert(input_ids,all_audio_data,token_type_ids, attention_mask, output_all_encoded_layers=True)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        pooled_output,text_att,fusion_att = self.BertFinetun(encoder_lastoutput,pooled_output,all_audio_data,video_data,extend_mask)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        if labels is not None:\n",
        "            loss = 0.5 * (logits.view(-1) - labels) ** 2 \n",
        "            return loss\n",
        "        else:\n",
        "            return logits,text_att,fusion_att"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcuNwge058jv",
        "outputId": "7a4ab719-1d10-4ba2-d1ff-024b3272631a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BertFinetun(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertFinetun, self).__init__()\n",
        "        self.proj_t = nn.Conv1d(768,30, kernel_size=1, padding=0, bias=False)\n",
        "        self.proj_a = nn.Conv1d(128,30, kernel_size=1, padding=0, bias=False)\n",
        "\n",
        "        ch1 = 32\n",
        "        k1 = (4, 4, 4)  # 3d kernel size\n",
        "        s1 = (2, 2, 1)  # 3d strides\n",
        "        pd1 = (1, 1, 2)  # 3d padding\n",
        "        self.proj_v_1 = nn.Conv3d(in_channels=5, out_channels=ch1, \n",
        "                                kernel_size=k1, stride=s1,\n",
        "                                padding=pd1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(ch1)\n",
        "        self.drop_video = nn.Dropout3d(0.1)\n",
        "        self.lstm = nn.LSTM(input_size=6656, hidden_size=30, num_layers=2, batch_first=True)\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "        self.audio_weight_1 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n",
        "        self.text_weight_1 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n",
        "        self.video_weight_1 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n",
        "        self.bias = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n",
        "        self.audio_weight_1.data.fill_(1)\n",
        "        self.text_weight_1.data.fill_(1)\n",
        "        self.video_weight_1.data.fill_(1)\n",
        "        self.bias.data.fill_(0)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.dense = nn.Linear(768, 768)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.LayerNorm1 = BertLayerNorm(768)\n",
        "        \n",
        "        \n",
        "    def forward(self, hidden_states,pooled_output,audio_data,video_data,attention_mask):\n",
        "        attention_mask = attention_mask.squeeze(1)\n",
        "        attention_mask_ = attention_mask.permute(0, 2, 1)\n",
        "        text_data = hidden_states\n",
        "        text_data = text_data.transpose(1, 2)\n",
        "        text_data = self.proj_t(text_data)\n",
        "        text_data = text_data.transpose(1, 2)\n",
        "        text_data_1 = text_data.reshape(-1).cpu().detach().numpy()\n",
        "        weights = np.sqrt(np.linalg.norm(text_data_1,ord=2))\n",
        "        text_data = text_data/weights\n",
        "\n",
        "        audio_data = audio_data.transpose(1, 2)\n",
        "        audio_data = self.proj_a(audio_data)\n",
        "        audio_data = audio_data.transpose(1, 2)\n",
        "\n",
        "        video_data = self.proj_v_1(video_data)\n",
        "        video_data = self.bn1(video_data)\n",
        "\n",
        "        # print(video_data.shape)\n",
        "        video_data = video_data.reshape((video_data.shape[0], 50, 6656))\n",
        "        # video_data = video_data.view(video_data.size(0), -1)\n",
        "        # print(video_data.shape)\n",
        "\n",
        "        video_data, (_, _) = self.lstm(video_data)\n",
        "        # print(video_data.shape, text_data.shape, audio_data.shape)\n",
        "        \n",
        "        video_att = torch.matmul(video_data,video_data.transpose(-1, -2))\n",
        "        video_att = self.activation(video_att)\n",
        "\n",
        "        text_att = torch.matmul(text_data,text_data.transpose(-1, -2))\n",
        "        text_att1 = self.activation(text_att)\n",
        "\n",
        "        audio_att = torch.matmul(audio_data,audio_data.transpose(-1, -2))\n",
        "        audio_att = self.activation(audio_att)\n",
        "\n",
        "        # print(video_att.shape, text_att.shape, audio_att.shape)\n",
        "\n",
        "        audio_weight_1 = self.audio_weight_1\n",
        "        text_weight_1 = self.text_weight_1\n",
        "        video_weight_1 = self.video_weight_1\n",
        "        bias = self.bias\n",
        "        \n",
        "        fusion_att = text_weight_1 * text_att1 + audio_weight_1 * audio_att\n",
        "        # print(fusion_att.shape, video_weight_1.shape, video_att.shape)\n",
        "\n",
        "        fusion_att = fusion_att + video_weight_1 * video_att + bias \n",
        "\n",
        "        fusion_att1 = self.activation(fusion_att)\n",
        "        fusion_att = fusion_att+ attention_mask+ attention_mask_\n",
        "        fusion_att = nn.Softmax(dim=-1)(fusion_att)\n",
        "        fusion_att = self.dropout1(fusion_att)\n",
        "        \n",
        "        fusion_data = torch.matmul(fusion_att,hidden_states)\n",
        "        fusion_data = fusion_data+hidden_states\n",
        "\n",
        "        hidden_states_new = self.dense(fusion_data)\n",
        "        hidden_states_new = self.dropout(hidden_states_new)\n",
        "        hidden_states_new = self.LayerNorm1(hidden_states_new)\n",
        "        hidden_states_new = hidden_states_new[:,0]\n",
        "        return hidden_states_new,text_att1,fusion_att1"
      ],
      "metadata": {
        "id": "Go_g1yot8WOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!unzip uncased_L-12_H-768_A-12.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnAsd9C79JqF",
        "outputId": "bcf1fc7e-eafe-4add-972e-2208f3a41bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-16 08:20:42--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.4.128, 74.125.24.128, 172.217.194.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.4.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   186MB/s    in 2.1s    \n",
            "\n",
            "2022-10-16 08:20:44 (186 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG_NAME = '/content/uncased_L-12_H-768_A-12/bert_config.json'\n",
        "WEIGHTS_NAME = '/content/drive/MyDrive/ИТМО/НИР3/notebooks/pytorch_model.bin'"
      ],
      "metadata": {
        "id": "ImCCjds49HDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", cache_dir=None, num_labels = 7)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ33o1XF6LA_",
        "outputId": "257be694-5297-4fd1-dae4-5f28ee83112a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 407873900/407873900 [00:33<00:00, 12154456.33B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns    \n",
        "\n",
        "\n",
        "def accuracy_7(out, labels):\n",
        "    return np.sum(np.round(out) == np.round(labels)) / float(len(labels))\n",
        "\n",
        "\n",
        "def multi_metrics_for_valid_with_confusion(y_pred_tags, y_tags):\n",
        "    # print(y_pred_tags)\n",
        "    # print(y_tags)\n",
        "    y_pred_tags = np.round(y_pred_tags)\n",
        "    y_tags = np.round(y_tags)\n",
        "    acc = accuracy_score(y_tags, y_pred_tags, normalize=True)\n",
        "    prec = precision_score(y_tags, y_pred_tags, average='weighted')\n",
        "    f1 = f1_score(y_tags, y_pred_tags, average='weighted')\n",
        "    rec = recall_score(y_tags, y_pred_tags, average='weighted')\n",
        "    report = classification_report(y_tags, y_pred_tags, output_dict=True)\n",
        "    labels = ['-3', '-2', '-1', '0', '1', '2', '3']\n",
        "    matrix = confusion_matrix(y_tags, y_pred_tags)\n",
        "\n",
        "    acc7 = accuracy_7(y_pred_tags, y_tags)\n",
        "    \n",
        "    return acc, acc7, prec, f1, rec, report, matrix\n",
        "\n",
        "def evaluate(model, dataloader, criterion):\n",
        "    epoch_loss = 0.0\n",
        "    y_preds_tags_array = []\n",
        "    y_tags_array = []\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for idx, (video, audio, text, attr, labels) in enumerate(dataloader):\n",
        "            # features = features.unsqueeze(1)\n",
        "            \n",
        "            video = video.to(device)\n",
        "            text = text.to(device)\n",
        "            audio = audio.to(device)\n",
        "            attr = attr.to(device)\n",
        "            labels = labels.to(device) \n",
        "            \n",
        "            predictions = model(text, audio, video, attention_mask=attr)[0]\n",
        "            \n",
        "            # loss = torch.nn.functional.cross_entropy(predictions, labels)\n",
        "\n",
        "            # _, y_pred_tags = torch.max(predictions, dim = 1)\n",
        "            # _, y_tags = torch.max(labels, dim = 1)\n",
        "            # y_pred_tags = y_pred_tags.cpu().data\n",
        "            # y_tags = y_tags.cpu().data\n",
        "            predictions = predictions.cpu().data\n",
        "            predictions = [predictions[i][0].item() for i in range(len(predictions))]\n",
        "            labels = labels.cpu().data\n",
        "            labels = [labels[i].item() for i in range(len(labels))]\n",
        "\n",
        "            y_preds_tags_array.extend(predictions)\n",
        "            y_tags_array.extend(labels)\n",
        "\n",
        "            # epoch_loss += loss.item()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    acc, acc_7, prec, f1, rec, report, matrix = multi_metrics_for_valid_with_confusion(y_preds_tags_array, y_tags_array)\n",
        "        \n",
        "    return epoch_loss / len(dataloader), acc, acc_7, prec, f1, rec, report, matrix"
      ],
      "metadata": {
        "id": "We0Eo_A46USC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from torch import optim\n",
        "\n",
        "N_EPOCHS = 80\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 2e-6, eps = 1e-8)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "best_valid_acc = 0.0\n",
        "best_valid_acc_7 = 0.0\n",
        "best_valid_prec = 0.0\n",
        "best_valid_f1 = 0.0\n",
        "best_valid_rec = 0.0\n",
        "result_report = {}\n",
        "result_matrix = {}\n",
        "\n",
        "for epoch in tqdm(range(N_EPOCHS)):\n",
        "    train_loss, train_acc, train_prec, train_f1, train_rec, _ = train(model, train_dataloader, optimizer, criterion)\n",
        "    valid_loss, valid_acc, valid_acc_7, valid_prec, valid_f1, valid_rec, report, matrix = evaluate(model, valid_dataloader, criterion)\n",
        "\n",
        "    if valid_acc >= best_valid_acc:\n",
        "        result_report = report\n",
        "        result_matrix = matrix\n",
        "    \n",
        "    best_valid_acc = max(best_valid_acc, valid_acc)\n",
        "    best_valid_acc_7 = max(best_valid_acc_7, valid_acc_7)\n",
        "    best_valid_prec = max(best_valid_prec, valid_prec)\n",
        "    best_valid_rec = max(best_valid_rec, valid_rec)\n",
        "    best_valid_f1 = max(best_valid_f1, valid_f1)\n",
        "\n",
        "\n",
        "print(f\"Best valid acc = {best_valid_acc}\")\n",
        "print(f\"Best valid acc 7 = {best_valid_acc_7}\")\n",
        "print(f\"Best valid prec = {best_valid_prec}\")\n",
        "print(f\"Best valid f1 = {best_valid_f1}\")\n",
        "print(f\"Best valid rec = {best_valid_rec}\")\n",
        "print(f'Classification report = \\n {json.dumps(result_report, sort_keys=True, indent=4)}')\n",
        "\n",
        "ax = plt.subplot()\n",
        "sns.heatmap(result_matrix, annot=True, fmt='g', ax=ax);\n",
        "labels = ['-3', '-2', '-1', '0', '1', '2', '3']\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fh0ZyF_xBJ58",
        "outputId": "65c7a92f-a17f-4133-8133-22ea65a4e368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [11:49<00:00, 35.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best valid acc = 0.4759825327510917\n",
            "Best valid acc 7 = 0.4759825327510917\n",
            "Best valid prec = 0.4855634809853714\n",
            "Best valid f1 = 0.4725090563866299\n",
            "Best valid rec = 0.4759825327510917\n",
            "Classification report = \n",
            " {\n",
            "    \"0.0\": {\n",
            "        \"f1-score\": 0.30769230769230765,\n",
            "        \"precision\": 0.6666666666666666,\n",
            "        \"recall\": 0.2,\n",
            "        \"support\": 10\n",
            "    },\n",
            "    \"1.0\": {\n",
            "        \"f1-score\": 0.5625,\n",
            "        \"precision\": 0.5294117647058824,\n",
            "        \"recall\": 0.6,\n",
            "        \"support\": 30\n",
            "    },\n",
            "    \"2.0\": {\n",
            "        \"f1-score\": 0.3714285714285715,\n",
            "        \"precision\": 0.34210526315789475,\n",
            "        \"recall\": 0.40625,\n",
            "        \"support\": 32\n",
            "    },\n",
            "    \"3.0\": {\n",
            "        \"f1-score\": 0.42696629213483145,\n",
            "        \"precision\": 0.475,\n",
            "        \"recall\": 0.3877551020408163,\n",
            "        \"support\": 49\n",
            "    },\n",
            "    \"4.0\": {\n",
            "        \"f1-score\": 0.47500000000000003,\n",
            "        \"precision\": 0.4634146341463415,\n",
            "        \"recall\": 0.48717948717948717,\n",
            "        \"support\": 39\n",
            "    },\n",
            "    \"5.0\": {\n",
            "        \"f1-score\": 0.5294117647058824,\n",
            "        \"precision\": 0.5192307692307693,\n",
            "        \"recall\": 0.54,\n",
            "        \"support\": 50\n",
            "    },\n",
            "    \"6.0\": {\n",
            "        \"f1-score\": 0.5500000000000002,\n",
            "        \"precision\": 0.5238095238095238,\n",
            "        \"recall\": 0.5789473684210527,\n",
            "        \"support\": 19\n",
            "    },\n",
            "    \"accuracy\": 0.4759825327510917,\n",
            "    \"macro avg\": {\n",
            "        \"f1-score\": 0.4604284194230847,\n",
            "        \"precision\": 0.5028055173881539,\n",
            "        \"recall\": 0.4571617082344795,\n",
            "        \"support\": 229\n",
            "    },\n",
            "    \"weighted avg\": {\n",
            "        \"f1-score\": 0.4725090563866299,\n",
            "        \"precision\": 0.4836614767446397,\n",
            "        \"recall\": 0.4759825327510917,\n",
            "        \"support\": 229\n",
            "    }\n",
            "}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from torch import optim\n",
        "\n",
        "N_EPOCHS = 100\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 2e-6, eps = 1e-8)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "best_valid_acc = 0.0\n",
        "best_valid_acc_7 = 0.0\n",
        "best_valid_prec = 0.0\n",
        "best_valid_f1 = 0.0\n",
        "best_valid_rec = 0.0\n",
        "result_report = {}\n",
        "result_matrix = {}\n",
        "\n",
        "for epoch in tqdm(range(N_EPOCHS)):\n",
        "    train_loss, train_acc, train_prec, train_f1, train_rec, _ = train(model, train_dataloader, optimizer, criterion)\n",
        "    valid_loss, valid_acc, valid_acc_7, valid_prec, valid_f1, valid_rec, report, matrix = evaluate(model, valid_dataloader, criterion)\n",
        "\n",
        "    if valid_acc >= best_valid_acc:\n",
        "        result_report = report\n",
        "        result_matrix = matrix\n",
        "    \n",
        "    best_valid_acc = max(best_valid_acc, valid_acc)\n",
        "    best_valid_acc_7 = max(best_valid_acc_7, valid_acc_7)\n",
        "    best_valid_prec = max(best_valid_prec, valid_prec)\n",
        "    best_valid_rec = max(best_valid_rec, valid_rec)\n",
        "    best_valid_f1 = max(best_valid_f1, valid_f1)\n",
        "\n",
        "\n",
        "print(f\"Best valid acc = {best_valid_acc}\")\n",
        "print(f\"Best valid acc 7 = {best_valid_acc_7}\")\n",
        "print(f\"Best valid prec = {best_valid_prec}\")\n",
        "print(f\"Best valid f1 = {best_valid_f1}\")\n",
        "print(f\"Best valid rec = {best_valid_rec}\")\n",
        "print(f'Classification report = \\n {json.dumps(result_report, sort_keys=True, indent=4)}')\n",
        "\n",
        "ax = plt.subplot()\n",
        "sns.heatmap(result_matrix, annot=True, fmt='g', ax=ax);\n",
        "labels = ['-3', '-2', '-1', '0', '1', '2', '3']\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0IJCQmvWENsC",
        "outputId": "4cf8497b-5e4b-46d5-faf4-38d40f9b0e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [11:49<00:00, 35.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best valid acc = 0.4847161572052402\n",
            "Best valid acc 7 = 0.4847161572052402\n",
            "Best valid prec = 0.49840261209067915\n",
            "Best valid f1 = 0.4819101083650754\n",
            "Best valid rec = 0.4847161572052402\n",
            "Classification report = \n",
            " {\n",
            "    \"0.0\": {\n",
            "        \"f1-score\": 0.30769230769230765,\n",
            "        \"precision\": 0.6666666666666666,\n",
            "        \"recall\": 0.2,\n",
            "        \"support\": 10\n",
            "    },\n",
            "    \"1.0\": {\n",
            "        \"f1-score\": 0.5172413793103449,\n",
            "        \"precision\": 0.5357142857142857,\n",
            "        \"recall\": 0.5,\n",
            "        \"support\": 30\n",
            "    },\n",
            "    \"2.0\": {\n",
            "        \"f1-score\": 0.39473684210526316,\n",
            "        \"precision\": 0.3409090909090909,\n",
            "        \"recall\": 0.46875,\n",
            "        \"support\": 32\n",
            "    },\n",
            "    \"3.0\": {\n",
            "        \"f1-score\": 0.45977011494252873,\n",
            "        \"precision\": 0.5263157894736842,\n",
            "        \"recall\": 0.40816326530612246,\n",
            "        \"support\": 49\n",
            "    },\n",
            "    \"4.0\": {\n",
            "        \"f1-score\": 0.45333333333333337,\n",
            "        \"precision\": 0.4722222222222222,\n",
            "        \"recall\": 0.4358974358974359,\n",
            "        \"support\": 39\n",
            "    },\n",
            "    \"5.0\": {\n",
            "        \"f1-score\": 0.5504587155963302,\n",
            "        \"precision\": 0.5084745762711864,\n",
            "        \"recall\": 0.6,\n",
            "        \"support\": 50\n",
            "    },\n",
            "    \"6.0\": {\n",
            "        \"f1-score\": 0.6,\n",
            "        \"precision\": 0.5714285714285714,\n",
            "        \"recall\": 0.631578947368421,\n",
            "        \"support\": 19\n",
            "    },\n",
            "    \"accuracy\": 0.4847161572052402,\n",
            "    \"macro avg\": {\n",
            "        \"f1-score\": 0.4690332418543011,\n",
            "        \"precision\": 0.517390171812244,\n",
            "        \"recall\": 0.4634842355102827,\n",
            "        \"support\": 229\n",
            "    },\n",
            "    \"weighted avg\": {\n",
            "        \"f1-score\": 0.4819101083650754,\n",
            "        \"precision\": 0.49840261209067915,\n",
            "        \"recall\": 0.4847161572052402,\n",
            "        \"support\": 229\n",
            "    }\n",
            "}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}